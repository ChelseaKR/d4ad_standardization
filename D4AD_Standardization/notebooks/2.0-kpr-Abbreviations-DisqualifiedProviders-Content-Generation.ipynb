{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# todo: set as env variable for raw main table\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "\n",
    "columns = [\n",
    "    \"NAME\",\n",
    "    \"NAME_1\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "df = pd.read_excel(rootpath + filepath, usecols=columns)\n",
    "df.PREREQUISITES.fillna(' ', inplace=True) # space so that matches match\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# for test dev purposes, let's focus on a really small subset\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "N = 20\n",
    "random_state = 42\n",
    "small_df = df.sample(n=N, random_state=random_state)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done2\n"
    }
   ],
   "source": [
    "def batches_of_docs(df, column_index=0, nlp=nlp, batch_size=BATCH_SIZE, disable=[\"parser\",\"ner\", \"entity_linker\"]):\n",
    "    yield nlp.pipe(df.iloc[:,column_index].values,\n",
    "                   batch_size=batch_size,\n",
    "                   disable=disable)\n",
    "\n",
    "def contents_of(matches, doc, matcher_spans_content=False):\n",
    "    # if matcher_spans_content=False then the matcher\n",
    "    # indicates where content is not and we span the\n",
    "    # regions excluded by the matcher\n",
    "    if not matcher_spans_content:\n",
    "        match_start = 0\n",
    "        for match in matches:\n",
    "            match_end = match[1]\n",
    "            if match[1] != -1:\n",
    "                yield doc[match_start:match_end]\n",
    "            match_start = match[2]\n",
    "        if match_start != -1:\n",
    "            yield doc[match_start:]\n",
    "\n",
    "\n",
    "patterns =\\\n",
    "    [\n",
    "        # these break up small_df.iloc[0] into unstandardized tokens\n",
    "        #[{'POS': 'PUNCT'}],  # fails in later samples\n",
    "        [{'POS': 'CCONJ'}],\n",
    "        # modifiction that breaks up small_df.iloc[7]\n",
    "        [{'ORTH': '/'}],\n",
    "        # modifiction that combines small_df.iloc[15], [1]\n",
    "        [{'ORTH': ','}],\n",
    "        # modifiction seen generally past 50 or os\n",
    "        [{'ORTH': ';'}],\n",
    "        # TODO: fix this to work, i could be special casing too early/improperly\n",
    "        # modifiction seen random_state*2\n",
    "        [{'IS_SPACE': True}], # captures present spaces after tokenizations\n",
    "    ]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DoNotStandardize\", patterns)\n",
    "\n",
    "interimpath = \"./D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"prereqs.csv\"\n",
    "\n",
    " \n",
    "the_df = df # could be small_df\n",
    "\n",
    "def write_df_content(the_df, column_index=5, matcher=matcher, interimpath=interimpath, content_is=content_is):\n",
    "    # We write out batches of prereqs to disk for downstream analyses, class induction\n",
    "    # note: I am not sure why batch_number doesn't increment with nlp.pipe yeilds in batches of docs\n",
    "    for batch_number, a_batch in enumerate(batches_of_docs(the_df, batch_size= 50, column_index=column_index)):\n",
    "        number_docs_per_batch = 50\n",
    "        content_path = rootpath + interimpath + \"{}_{}\".format(batch_number, content_is)\n",
    "        for doc_number, (doc, matches) in enumerate(matcher.pipe(a_batch, return_matches=True, batch_size=50)):\n",
    "            direct_doc_index = batch_number*number_docs_per_batch + doc_number\n",
    "\n",
    "            print('\\t ... adding ', doc.text[:80], '...', 'doc_number ', doc_number, ' batch number ', batch_number)\n",
    "\n",
    "            content_exists = not os.path.isfile(content_path)\n",
    "            with open(content_path, 'a') as csv: # append to dataframe containing content from batch_number docs\n",
    "                pd.DataFrame(\n",
    "                    data=\\\n",
    "                        {\n",
    "                            'content': contents_of(matches, doc),\n",
    "                            'CIPCODE': the_df.iloc[direct_doc_index].CIPCODE,\n",
    "                            'PREREQUISITES': the_df.iloc[direct_doc_index].PREREQUISITES,\n",
    "                            'batch_number': batch_number,\n",
    "                            'doc_number': doc_number\n",
    "                        }\n",
    "                ).to_csv(\n",
    "                    csv,\n",
    "                    index = False,\n",
    "                    chunksize = 10000,\n",
    "                    header=content_exists\n",
    "                )\n",
    "print('done2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "interimpath = \"./D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"state_comments.csv\"\n",
    "\n",
    "the_df = df\n",
    "the_df = the_df.dropna(subset=['STATECOMMENTS'])\n",
    "\n",
    "columns = ['STATECOMMENTS', 'PROVIDERID', 'APPROVINGAGENCYID', 'CIPCODE']\n",
    "\n",
    "the_df.to_csv(\n",
    "    rootpath + interimpath + \"{}\".format(content_is),\n",
    "                    index = False,\n",
    "                    chunksize = 10000,\n",
    "                    columns=columns)\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)",
   "language": "python",
   "name": "python37264bitd4adstandardizationpipenva2728973f1f441dc85d2ef2a14ed1d86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}