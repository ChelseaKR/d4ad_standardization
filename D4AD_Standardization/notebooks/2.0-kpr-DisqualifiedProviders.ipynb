{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# todo: set as env variable for raw main table\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "\n",
    "columns = [\n",
    "    \"NAME\",\n",
    "    \"NAME_1\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\"\n",
    "]\n",
    "\n",
    "df = pd.read_excel(rootpath + filepath, usecols=columns)\n",
    "df.PREREQUISITES.fillna('', inplace=True)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# for test dev purposes, let's focus on a really small subset\n",
    "BATCH_SIZE = 50\n",
    "N = 20\n",
    "random_state = 42\n",
    "small_df = df.sample(n=N, random_state=random_state)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... on a batch ... 0\n",
      "\t ... addded High School Diploma/G.E.D. or Ability To Benefit ...\n",
      "\t ... addded High School Diploma/G.E.D. or Ability To Benefit, keyboarding - 25 WPM ...\n",
      "\t ... addded High School Diploma or G.E.D. or Ability To Benefit, work history ...\n",
      "\t ... addded High School Diploma or G.E.D. or Ability To Benefit, strong work history ...\n",
      "\t ... addded High School Diploma or G.E.D. or Ability To Benefit ...\n",
      "\t ... addded None ...\n",
      "\t ... addded None ...\n",
      "\t ... addded High School Diploma or GED ...\n",
      "\t ... addded None ...\n",
      "\t ... addded None ...\n",
      "\t ... addded Keyboarding skills & Intro to Computers. ...\n",
      "\t ... addded None ...\n",
      "\t ... addded Basic Math Skills ...\n",
      "\t ... addded High School Diploma or GED ...\n",
      "\t ... addded Essentially none ...\n",
      "\t ... addded Essentially None ...\n",
      "\t ... addded Essentially none.. Concept of LAN/WAN advantageous ...\n",
      "\t ... addded Essentially None ...\n",
      "\t ... addded A+ ...\n",
      "\t ... addded SQL and Network+ ...\n",
      "\t ... addded SQL and Network+ ...\n",
      "\t ... addded SQL ...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "[E040] Attempt to access token at -1, max length 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-508b4276f4af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t ... addded'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/d4ad_standardization-rpRrE5oG/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[1;32m   3165\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3166\u001b[0m         )\n\u001b[0;32m-> 3167\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/d4ad_standardization-rpRrE5oG/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             )\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/d4ad_standardization-rpRrE5oG/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/d4ad_standardization-rpRrE5oG/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mlibwriters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__repr__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.text.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mspan.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtoken.pxd\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.cinit\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: [E040] Attempt to access token at -1, max length 0."
     ]
    }
   ],
   "source": [
    "def batches_of_docs(df, column_index=0, nlp=nlp, batch_size=BATCH_SIZE, disable=[\"parser\",\"ner\", \"entity_linker\"]):\n",
    "    yield nlp.pipe(df.iloc[:,column_index].values,\n",
    "                   batch_size=batch_size,\n",
    "                   disable=disable)\n",
    "\n",
    "def contents_of(matches, doc, matcher_spans_content=False):\n",
    "    # if matcher_spans_content=False then the matcher\n",
    "    # indicates where content is not and we span the\n",
    "    # regions excluded by the matcher\n",
    "    if not matcher_spans_content:\n",
    "        match_start = 0\n",
    "        for match in matches:\n",
    "            match_end = match[1]\n",
    "            if match[1] != -1:\n",
    "                yield doc[match_start:match_end]\n",
    "            match_start = match[2]\n",
    "        if match_start != -1:\n",
    "            yield doc[match_start:]\n",
    "\n",
    "\n",
    "patterns =\\\n",
    "    [\n",
    "        # these break up small_df.iloc[0] into unstandardized tokens\n",
    "        #[{'POS': 'PUNCT'}],  # fails in later samples\n",
    "        [{'POS': 'CCONJ'}],\n",
    "        # modifiction that breaks up small_df.iloc[7]\n",
    "        [{'ORTH': '/'}],\n",
    "        # modifiction that combines small_df.iloc[15], [1]\n",
    "        [{'ORTH': ','}],\n",
    "        # modifiction seen generally past 50 or os\n",
    "        [{'ORTH': ';'}],\n",
    "        # TODO: fix this to work, i could be special casing too early/improperly\n",
    "        # modifiction seen random_state*2\n",
    "        [{'IS_SPACE': True}], # captures present spaces after tokenizations\n",
    "    ]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DoNotStandardize\", patterns)\n",
    "\n",
    "interimpath = \"./D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"prereqs.csv\"\n",
    "\n",
    "import os \n",
    "the_df = df # could be small_df\n",
    "\n",
    "# We write out batches of prereqs to disk for downstream analyses, class induction\n",
    "for batch_number, a_batch in enumerate(batches_of_docs(the_df, batch_size= 50, column_index=10)):\n",
    "#for batch_number, a_batch in enumerate(batches_of_docs(small_df, column_index=10)):\n",
    "    number_docs_per_batch = 50\n",
    "    print('... on a batch ...', batch_number)\n",
    "    content_path = rootpath + interimpath + \"{}_{}\".format(batch_number, content_is)\n",
    "    \n",
    "    for doc_number, (doc, matches) in enumerate(matcher.pipe(a_batch, return_matches=True, batch_size=50)):\n",
    "        direct_doc_index = batch_number*number_docs_per_batch + doc_number\n",
    "\n",
    "        content_exists = not os.path.isfile(content_path)\n",
    "        with open(content_path, 'a') as csv: # append to dataframe containing content from batch_number docs\n",
    "            pd.DataFrame(\n",
    "                data=\\\n",
    "                    {\n",
    "                        'content': contents_of(matches, doc),\n",
    "                        'CIPCODE': the_df.iloc[direct_doc_index].CIPCODE,\n",
    "                        'PREREQUISITES': the_df.iloc[direct_doc_index].PREREQUISITES,                    \n",
    "                        'batch_number': batch_number,\n",
    "                        'doc_number': doc_number\n",
    "                    }\n",
    "            ).to_csv(\n",
    "                csv,\n",
    "                index = False,\n",
    "                chunksize = 10000,\n",
    "                header=content_path\n",
    "            )\n",
    "        print('\\t ... addded', doc.text[:80], '...')\n",
    "    \n",
    "    if batch_number > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
