{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# todo: set as env variable for raw main table\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "\n",
    "columns = [\n",
    "    \"NAME\",\n",
    "    \"NAME_1\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\"\n",
    "]\n",
    "\n",
    "df = pd.read_excel(rootpath + filepath, usecols=columns)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# for test dev purposes, let's focus on a really small subset\n",
    "BATCH_SIZE = 50\n",
    "N = 20\n",
    "random_state = 42\n",
    "small_df = df.sample(n=N, random_state=random_state)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... on a batch ... 0\n",
      "\t ... addded H.S. diploma or GED. If not must pass the Wonderlic test. ...\n",
      "\t ... addded Windows Server 2003 and 2008 Terminal Services Microsoft SQL Server 2005 with Re ...\n",
      "\t ... addded High School Diploma/GED or ATB ...\n",
      "\t ... addded  High school diploma, GED, Degree, 1500 hours in completing projects from work a ...\n",
      "\t ... addded None ...\n",
      "\t ... addded HS Diploma/GED ...\n",
      "\t ... addded High School Diploma or GED ...\n",
      "\t ... addded Background check / High School Diploma / Students must be at least 18 years of a ...\n",
      "\t ... addded High School Graduate ...\n",
      "\t ... addded Limited english speaking ...\n",
      "\t ... addded Advanced Computer Knowledge ...\n",
      "\t ... addded Computer Concepts ...\n",
      "\t ... addded High School Diploma ...\n",
      "\t ... addded High School Diploma or GED ...\n",
      "\t ... addded HS Diploma, GED, ATB test ...\n",
      "\t ... addded NYS 8 hour pre-assignment certification, 18+ age, valid photo id, valid social s ...\n",
      "\t ... addded H.S. Diploma, GED, 18 Years of Age or Older ...\n",
      "\t ... addded Personal Interview and Entrance Test ...\n",
      "\t ... addded GED or High School Equivalent ...\n",
      "\t ... addded none ...\n"
     ]
    }
   ],
   "source": [
    "def batches_of_docs(df, column_index=0, nlp=nlp, batch_size=BATCH_SIZE, disable=[\"parser\",\"ner\", \"entity_linker\"]):\n",
    "    yield nlp.pipe(df.iloc[:,column_index].values,\n",
    "                   batch_size=batch_size,\n",
    "                   disable=disable)\n",
    "\n",
    "def contents_of(matches, doc, matcher_spans_content=False):\n",
    "    # if matcher_spans_content=False then the matcher\n",
    "    # indicates where content is not and we span the\n",
    "    # regions excluded by the matcher\n",
    "    if not matcher_spans_content:\n",
    "        match_start = 0\n",
    "        for match in matches:\n",
    "            match_end = match[1]\n",
    "            yield doc[match_start:match_end]\n",
    "            match_start = match[2]\n",
    "        yield doc[match_start:]\n",
    "\n",
    "\n",
    "patterns =\\\n",
    "    [\n",
    "        # these break up small_df.iloc[0] into unstandardized tokens\n",
    "        #[{'POS': 'PUNCT'}],  # fails in later samples\n",
    "        [{'POS': 'CCONJ'}],\n",
    "        # modifiction that breaks up small_df.iloc[7]\n",
    "        [{'ORTH': '/'}],\n",
    "        # modifiction that combines small_df.iloc[15], [1]\n",
    "        [{'ORTH': ','}],\n",
    "        # modifiction seen generally past 50 or os\n",
    "        [{'ORTH': ';'}],\n",
    "        # TODO: fix this to work, i could be special casing too early/improperly\n",
    "        # modifiction seen random_state*2\n",
    "        [{'IS_SPACE': True}], # captures present spaces after tokenizations\n",
    "    ]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DoNotStandardize\", patterns)\n",
    "\n",
    "interimpath = \"./D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"prereqs.csv\"\n",
    "\n",
    "# We write out batches of prereqs to disk for downstream analyses, class induction\n",
    "for batch_number, a_batch in enumerate(batches_of_docs(small_df, column_index=10)):\n",
    "    print('... on a batch ...', number)\n",
    "    content_path = rootpath + interimpath + \"{}_{}\".format(number, content_is)\n",
    "    \n",
    "    for doc_number, (doc, matches) in enumerate(matcher.pipe(a_batch, return_matches=True, batch_size=50)):\n",
    "        direct_doc_index = batch_number*doc_number + doc_number\n",
    "        with open(content_path, 'a') as csv: # append to dataframe containing content from batch_number docs\n",
    "            pd.DataFrame(\n",
    "                data=\\\n",
    "                    {\n",
    "                        'content': contents_of(matches, doc),\n",
    "                        'CIPCODE': small_df.iloc[direct_doc_index].CIPCODE,\n",
    "                        'PREREQUISITES': small_df.iloc[direct_doc_index].PREREQUISITES,                    \n",
    "                        'batch_number': number,\n",
    "                        'doc_number': doc_number\n",
    "                    }\n",
    "            ).to_csv(\n",
    "                csv,\n",
    "                index = False,\n",
    "                chunksize = 10000,\n",
    "                header=csv.tell()==0 # only add header if new file\n",
    "            )\n",
    "        print('\\t ... addded', doc.text[:80], '...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
