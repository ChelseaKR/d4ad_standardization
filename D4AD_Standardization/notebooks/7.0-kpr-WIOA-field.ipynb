{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import regex  # for better, more capabale regex api\n",
    "import os\n",
    "import zipfile\n",
    "import more_itertools\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # so we can peak at data and spot verify\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_descriptions_and_degree_funding_type\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done2\n"
    }
   ],
   "source": [
    "\n",
    "filepath = content_is + \".csv\" # builds off of notebook 6.0 work\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "df = pd.read_csv(rootpath + interimpath + filepath)\n",
    "print('done2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "training_context_regex =\\\n",
    "    \"\"\"\n",
    "    ((\\w+\\W+){0,8}        # first 8 or so words before\n",
    "     (?P<job_search>job[\\s\\b.].*?search|assist[\\w\\s\\b\\.].*?employ\\w*?\\b) # help w/ job search\n",
    "     (\\W+\\w+){0,8})       # and last 8 or so word after\n",
    "    \"\"\"\n",
    "\n",
    "wioa_indices = the_df['IS_WIOA'] == True\n",
    "the_df['has_minimum_six_months_search'] =\\\n",
    "    False\n",
    "the_df.loc[wioa_indices, 'minimum_six_months_search'] =\\\n",
    "    True\n",
    "\n",
    "# Extract job search context...\n",
    "job_search_length_mention =\\\n",
    "    the_df.loc[wioa_indices, 'DESCRIPTION']\\\n",
    "          .str\\\n",
    "          .extractall(pat=training_context_regex, flags=re.I|re.VERBOSE)[0]\n",
    "\n",
    "the_df['additional_job_search_duration_mention'] =\\\n",
    "    None\n",
    "\n",
    "# ... all mentions follow <numeric reference> ... <time duration> format\n",
    "# e.g. three hours per day, 4-week job readiness, Four week job search, etc.\n",
    "\n",
    "# We'll take the simpler approach of treating the duration as the anchor\n",
    "# and grab those statements that have a numeric 0, 1, 2, 3 or 4 words before it\n",
    "\n",
    "numbers = \"one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen\"\n",
    "durations = \"minute|day|week|month|year\"\n",
    "duration_regex =\\\n",
    "   f\"\"\"\n",
    "    (?P<numeric>\\d|{numbers})       # numeric reference ...\n",
    "    (?P<modifer>.*)                 # followed by content that might modify ...\n",
    "    (?P<base_duration>{durations})     # ... the base duration\n",
    "    \"\"\"\n",
    "\n",
    "job_search_length_mention =\\\n",
    "    job_search_length_mention.str.extract(\n",
    "        duration_regex,\n",
    "        flags=re.I|re.VERBOSE\n",
    "    ).replace('-', '')\\\n",
    "    .replace('week', 'weeks')\\\n",
    "    .dropna()\\\n",
    "    .droplevel('match')  # drop weird \n",
    "\n",
    "the_df.loc[job_search_length_mention.index,\n",
    "           'additional_job_search_duration_mention'] =\\\n",
    "               job_search_length_mention['numeric'] +\\\n",
    "               job_search_length_mention['modifer'] +\\\n",
    "               job_search_length_mention['base_duration']\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "We're writing ... Index(['IS_WIOA', 'Mentioned_Certificate', 'Mentioned_Associates',\n       'STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION',\n       'CLEANED_STANDARDIZED_NAME_1', 'STANDARDIZEDNAME', 'STANDARDIZEDNAME_1',\n       'DESCRIPTION', 'FEATURESDESCRIPTION', 'NAME_1', 'NAME',\n       'has_minimum_six_months_search', 'minimum_six_months_search',\n       'additional_job_search_duration_mention'],\n      dtype='object')\ndone\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = the_df\n",
    "\n",
    "print(\n",
    "    \"We're writing ...\",\n",
    "    write_out.columns\n",
    ")\n",
    "\n",
    "content_is = \"with_job_search_durations\"\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    write_out.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized Descriptions\",\n",
    "            index=False)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}