{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import regex  # for better, more capbale regex api\n",
    "import os\n",
    "import zipfile\n",
    "import more_itertools\n",
    "from itertools import chain\n",
    "import datetime\n",
    "import time\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # so we can peak at data and spot verify\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_descriptions_and_degree_funding_type\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done2\n"
    }
   ],
   "source": [
    "\n",
    "filepath = content_is + \".csv\" # builds off of notebook 6.0 work\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "df = pd.read_csv(rootpath + interimpath + filepath)\n",
    "print('done2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done1\n"
    }
   ],
   "source": [
    "# 2) We need to create a length of job search assistance field; WIOA, at minimum, offers\n",
    "# 6 months, but some counties or states require more; this possibly migth be mentioned\n",
    "# and we would like to extract that.\n",
    "\n",
    "# We first repeat the regex machinery from notebook 6.0 so taht we can\n",
    "# more easily search and find rows with training realted field data.\n",
    "\n",
    "\n",
    "# TODO: try out basic .str.match from Pandas first, this machinery might not\n",
    "# be needed.\n",
    "def make_term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    mystr = left_regex + '(' +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' + right_regex\n",
    "    return mystr\n",
    "\n",
    "def make_grouped_regexes(replacement, left_regex=\"\", right_regex=\"\"):\n",
    "    return (make_term_grouped_regex(left_regex=left_regex,\n",
    "                                    term=key,\n",
    "                                    right_regex=right_regex)\\\n",
    "            for key in replacement.keys()\n",
    "    )\n",
    "\n",
    "def construct_map(label_mapper=label_mapper):\n",
    "    return {\n",
    "        **dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "    }\n",
    "\n",
    "abbrevation_pattern =\\\n",
    "    regex.compile(\n",
    "        \"(?p)\" +\n",
    "        \"|\".join(   # match words at start of string\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'[\\s:]')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words surrounded by spaces\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'\\s', right_regex=r'\\s')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words that make up entire fields, e.g. 'Nursing'\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'^', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words at end of string preceded by space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'[\\s/]', right_regex=r'$')\n",
    "        ) + \"|\" +\\\n",
    "        \"|\".join(   # match words within string that follow a slash, end with a space or slash\n",
    "            make_grouped_regexes(replacement_map, left_regex=r'/', right_regex=r'[\\s/]')\n",
    "        )\n",
    "    )\n",
    "\n",
    "def multiple_mapper(string):\n",
    "    return abbrevation_pattern.sub(\n",
    "        lambda x: \\\n",
    "        x.group().replace( # replace the found string\n",
    "            more_itertools.first_true(x.groups() # where the first matched group...\n",
    "        ),  replacement_map[more_itertools.first_true(x.groups())] # ... is replaced with the lookup\n",
    "    ), string)\n",
    "print('done1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'the_df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7cb40c3be668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mthe_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minimum_six_months'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mthe_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwioa_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'minimum_six_months'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'the_df' is not defined"
     ]
    }
   ],
   "source": [
    "training_regex =\\\n",
    "    \"\"\"\n",
    "    (job[\\s\\b.].*?search                # job search, mentioned in an order\n",
    "     |assist[\\w\\s\\b\\.].*?employ         # or assitance gaining employment\n",
    "     |employ[\\w\\s\\b\\.].*?assist)        # or employment assistance :)\n",
    "    \"\"\"\n",
    "\n",
    "get_surrounding_prior_words_regex =\\\n",
    "    \"\"\"\n",
    "    (?P<prior>\\w+\\W+){0,4}\\b\n",
    "    \"\"\"\n",
    "\n",
    "get_surrounding_after_words_regex =\\\n",
    "    \"\"\"\n",
    "    \\b(?P<after>\\W+\\w+){0,4})\n",
    "    \"\"\"\n",
    "\n",
    "the_regex = get_surrounding_prior_words_regex +\\\n",
    "            training_regex                    +\\\n",
    "            get_surrounding_after_words_regex\n",
    "\n",
    "the_df['minimum_six_months'] =\\\n",
    "    False\n",
    "\n",
    "the_df[wioa_indices, 'minimum_six_months'] =\\\n",
    "    True\n",
    "\n",
    "the_df['additional_search_mention'] =\\\n",
    "    None\n",
    "    \n",
    "# TODO: finish out this logic, should create a prior, after column\n",
    "# that contain time/date information that can be examined (e.g. \n",
    "# contains four, and month, so four months, etc.)\n",
    "wioa_indices = the_df['IS_WIOA'] == True\n",
    "mention_training_indices = the_df.loc[wioa_indices, 'DESCRIPTION']\\\n",
    "                                 .str\\\n",
    "                                 .extractall(the_regex, flags=re.I|re.VERBOSE)\n",
    "\n",
    "# Of those mentioning job searches, do they mention how long?\n",
    "how_long_regex =\\\n",
    "    \"\"\"\n",
    "     .*\\d\\d.*\n",
    "    |one\n",
    "    |three\n",
    "    |four\n",
    "    |five\n",
    "    |six\n",
    "    |seven\n",
    "    |eight\n",
    "    |nine\n",
    "    |ten\n",
    "    |eleven\n",
    "    |twelve\n",
    "    |thirteen\n",
    "    |fourteen\n",
    "    |fifteen\n",
    "    |sixteen\n",
    "    \"\"\"\n",
    "\n",
    "# I want\n",
    "#     0 a number\n",
    "#     0 followed by a length of time\n",
    "#     0  followed or preceded by a mention of the training regex\n",
    "\n",
    "the_df.loc[wioa_indices & mention_training_indices, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "We're writing ... Index(['IS_WOIA', 'Mentioned_Certificate', 'Mentioned_Associates',\n       'STANDARDIZED_DESCRIPTION', 'STANDARDIZED_FEATURESDESCRIPTION',\n       'CLEANED_STANDARDIZED_NAME_1', 'STANDARDIZEDNAME', 'STANDARDIZEDNAME_1',\n       'DESCRIPTION', 'FEATURESDESCRIPTION', 'NAME_1', 'NAME'],\n      dtype='object')\ndone\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = draft_output\n",
    "\n",
    "print(\n",
    "    \"We're writing ...\",\n",
    "    write_out.columns\n",
    ")\n",
    "\n",
    "content_is = \"with_job_search_durations\"\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    write_out.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized Descriptions\",\n",
    "            index=False)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}