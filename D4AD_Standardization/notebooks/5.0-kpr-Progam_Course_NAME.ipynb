{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "\n",
    "columns = [\n",
    "    \"NAME\",\n",
    "    \"NAME_1\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "df = pd.read_excel(rootpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "\n",
    "content_is = \"NAME_1_lookup_table.csv\"\n",
    "\n",
    "columns_to_save = ['STANDARDIZEDNAME1', 'NAME_1', 'PROVIDERID',\n",
    "                    'APPROVINGAGENCYID', 'CIPCODE']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8847553b674d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# see: https://www.careeronestop.org/Developers/Data/data-downloads.aspx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# need to stand up or use a db connector to import it, though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mONET_TOOLS_TECH_URL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"onet_tools_tech.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mCAREERONESTOP_CERTIFICATIONS_URL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"career_one_stop.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8847553b674d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# see: https://www.careeronestop.org/Developers/Data/data-downloads.aspx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# need to stand up or use a db connector to import it, though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mONET_TOOLS_TECH_URL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"onet_tools_tech.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mCAREERONESTOP_CERTIFICATIONS_URL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"career_one_stop.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_pydevd_bundle/pydevd_cython.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.vscode-server/extensions/ms-python.python-2020.8.103604/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, is_unhandled_exception)\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1845\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads_suspended_single_notification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify_thread_suspended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1846\u001b[0;31m                 \u001b[0mkeep_suspended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuspend_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_this_thread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mframes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.vscode-server/extensions/ms-python.python-2020.8.103604/pythonFiles/lib/python/debugpy/_vendored/pydevd/pydevd.py\u001b[0m in \u001b[0;36m_do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1881\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_current_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ONET_TOOLS_TECH_URL_NAME = (\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\", \"onet_tools_tech.csv\")\n",
    "CAREERONESTOP_CERTIFICATIONS_URL_NAME = (\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\", \"career_one_stop.zip\")\n",
    "\n",
    "filepath = rootpath + externalpath\n",
    "\n",
    "for dataset in (ONET_TOOLS_TECH_URL_NAME, CAREERONESTOP_CERTIFICATIONS_URL_NAME):\n",
    "    url, filename = dataset\n",
    "    print(\"running ...\", f'\\nwget -O {filepath+filename} {url}')\n",
    "    os.system(f'wget -O {filepath+filename} {url}')\n",
    "    print(\"filetype is\",  filename[-3:])\n",
    "\n",
    "    if filename[-3:] == 'zip':\n",
    "        with zipfile.ZipFile(filepath+filename,\"r\") as zip_ref:\n",
    "            zipdir = filepath+filename[:-4]\n",
    "            print(\"unzipping {} to ...\".format(filename), zipdir)\n",
    "            os.mkdir(zipdir)\n",
    "            zip_ref.extractall(zipdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# A) \n",
    "# The program or course name can start or end with a matching parenthesis. In these cases\n",
    "# we assume that no other matching parenthesis are present and apply \n",
    "# an appropriate regex for that...\n",
    "\n",
    "# First, set up standardized column with default values\n",
    "the_df[\"STANDARDIZEDNAME_1\"] = \"\"\n",
    "\n",
    "# ... then extract names for those with opening parens\n",
    "open_parenthesis_index = the_df.NAME_1.str[0] == '('\n",
    "open_parenthesis_regex = '''\n",
    "                (?P<paren>\\(.*\\)) # get the first parathesis\n",
    "                (?P<the_name>.*)  # then get the actual name\n",
    "                '''\n",
    "\n",
    "the_df.loc[open_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[open_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(open_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then extract names for those with closing parens\n",
    "close_parenthesis_index = the_df.NAME_1.str[-1] == ')'\n",
    "closing_parenthesis_regex = '''\n",
    "                (?P<the_name>.*)  # get the actual name\n",
    "                (?P<paren>\\(.*\\)) # get the last parathensis                \n",
    "                '''\n",
    "the_df.loc[close_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[close_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then we copy over content that has a internal parenthesis with those\n",
    "# parenthesis removed and ignore everything after, e.g. \"ABC (123) DEF\" --> \"ABC\"\n",
    "internal_parenthesis_index =\\\n",
    "    the_df['NAME_1'].str.contains('\\(|\\)', regex=True) &\\\n",
    "        ~(close_parenthesis_index|open_parenthesis_index)\n",
    "\n",
    "the_df.loc[internal_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[internal_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... finally, just copy over everything else\n",
    "no_parenthesis_index = ~(close_parenthesis_index |\\\n",
    "                         open_parenthesis_index  |\\\n",
    "                         internal_parenthesis_index)\n",
    "the_df.loc[no_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[no_parenthesis_index, 'NAME_1']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# 2)\n",
    "# So now we have silver version data of program, course names\n",
    "# from the cell above, in STANDARDIZEDNAME_1\n",
    "#\n",
    "# To make an incrementally better version we need to expand \n",
    "# abbreviations and acroynmns.\n",
    "\n",
    "# Here I identify two pretty common cases of acronymns and abbreviations:\n",
    "#   All caps\n",
    "#   Xx*. <- capitalized inital letter ending with a period\n",
    "\n",
    "# Now let's attempt to extract presumed acronyms and see if we can\n",
    "# directly label them. I assume there are far fewer unique abbreviations\n",
    "# so that a person can actually do this in a short amount of time\n",
    "flags = re.VERBOSE\n",
    "\n",
    "#  TODO: check if abbreviation labeled file already exists, if it does\n",
    "# we skip this portion\n",
    "\n",
    "# Pandas/Python doesn't like this verbose regex but likes other?\n",
    "# all_caps_regex = '''\n",
    "#                 \\b(?P<all_caps>[A-Z]+)  # Get all caps words\n",
    "#                 [\\s,:\\d]                # sit before a space, comma or digit\n",
    "#                 '''\n",
    "all_caps_regex = r'\\b(?P<all_caps>[A-Z]+)[\\s,:\\d]'\n",
    "\n",
    "dotted_word_regex = r'(?P<dot_abbreviation>[A-Z][a-z]+\\.)'\n",
    "dotted_word_regex =\\\n",
    "    \"\"\"\n",
    "    (?P<dot_abbreviation>[a-zA-Z][a-z]+\\.)\n",
    "    \"\"\"\n",
    "\n",
    "the_regexs = \"|\".join([all_caps_regex, dotted_word_regex])\n",
    "\n",
    "the_abbreviations =\\\n",
    "    the_df['STANDARDIZEDNAME_1'].str\\\n",
    "                                .extractall(\n",
    "                                    pat=the_regexs,\n",
    "                                    flags=flags)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Since we've run on the entire dataset we can now\n",
    "# flatten the dataframe, de-duplicate and then directly label\n",
    "\n",
    "len(the_abbreviations.all_caps.unique()) +\\\n",
    "    len(the_abbreviations.dot_abbreviation.unique()) # 1151\n",
    "\n",
    "# now we need to get the count of unique abbreviations so that we can\n",
    "# label in priority order. We also drop those abbreviations only occuring once\n",
    "# since they have a 1 / 26,660 chance of occuring (not worth our effort)\n",
    "\n",
    "# to properly label the all caps and abbreviations we need the \n",
    "# context in which they occur. Since we're mapping to one definition\n",
    "# we assume only the first instance is really needed and label off of that\n",
    "abbreviations_to_label =\\\n",
    "    pd.concat(\n",
    "        (the_abbreviations.drop_duplicates(\n",
    "            subset=['all_caps'],\n",
    "            keep='first')['all_caps'],\n",
    "         the_abbreviations.drop_duplicates(\n",
    "             subset=['dot_abbreviation'],\n",
    "             keep='first')['dot_abbreviation']\n",
    "        ),\n",
    "        axis=0\n",
    "    ).dropna()\\\n",
    "     .droplevel('match')\\\n",
    "     .reset_index() # so that index is a column\n",
    "\n",
    "abbreviations_to_label.rename(columns={'index':'the_df_index', 0:'abbreviation'}, inplace=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "ACTIONSCRIPT     1\nImpl.            1\nDev.             1\nSERVICES         1\nLSAT             1\nAWARENESS        1\nAFOT             1\nUCC              1\nCPS              1\nFt.              1\nPREVENTION       1\nSTANDARDIZED     1\n                ..\nSQF              1\nSalesforce.      1\nDEVELOPMENTAL    1\nAWCA             1\nSecr.            1\nOHSA             1\nMCTS             1\nCQT              1\nCLS              1\nWIOA             1\nCORPORATE        1\nNFPA             1\nName: abbreviation, Length: 845, dtype: int64"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "# we also filter out thosea abbreviations already labeled in a prior\n",
    "# session\n",
    "\n",
    "# Note: this is very dependent on the ordering of the source ETLP datafile\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "abbreviation_pickle = rootpath + interimpath + 'abbreviation_label.pickle'\n",
    "\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    expanded_labels = pd.read_pickle(abbreviation_pickle)\n",
    "    last_labeled_index = expanded_labels.index(None)\n",
    "    former_labels = abbreviations_to_label.abbreviation[:last_labeled_index] #set(already_labeled[:last_labeled_index])\n",
    "\n",
    "unseen_abbreviations =\\\n",
    "    abbreviations_to_label.query('abbreviation not in @former_labels')\n",
    "unseen_abbreviations.abbreviation.value_counts()\n",
    "# note I'm seeing 1 across the board, both when using not in and in\n",
    "#   which suggests that we leave these be for now because their occurence\n",
    "# is so rare out of 26,660, although the bulk may be significant; better\n",
    "# to circle back though\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c73250e462f54c9994809d2dafd9554b"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#  So now we manually label them and dump them here\n",
    "#  This is the procedure we follow\n",
    "#       A) if a capitalized word is an entire word, leave it alone (no label)\n",
    "#       B) provide a label for all dotted abbreviated words\n",
    "def display_func(row):\n",
    "    # Note: We use globally available the_df to get context, bad form I know\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"**Context:** \" +  the_df.loc[ row.the_df_index ].NAME_1 \\\n",
    "        +   \"\\n\\n\" + row.abbreviation\n",
    "        )\n",
    "    )\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x.abbreviation, y\n",
    "\n",
    "labelling_widget = ClassLabeller(\n",
    "    features=abbreviations_to_label,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['No Label'],\n",
    "    acquisition_function='entropy'\n",
    ")\n",
    "\n",
    "labelling_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "#  Every now and then, with the labels in had we simply output them (if a file doesn't already exist)\n",
    "# so that we can save them incrementally. We should manually rename older files; this should\n",
    "# be basically a 1 time process.\n",
    "\n",
    "# Temp, save work locally so we don't loooooose it! \n",
    "pickle.dump(labelling_widget.new_labels,\n",
    "            open(abbreviation_pickle, 'wb'))\n",
    "print('done')\n",
    "\n",
    "#sum([1 for label in labelling_widget.new_labels if label != None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                      STANDARDIZEDNAME_1  \\\n0                     Automated Office Systems Processor   \n1      Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...   \n2                Bus.Soft. App./Comp.Office Proc.Medical   \n3                         Microsoft Office Specialist II   \n4                      Computerized Financial Accounting   \n5                              Introduction to Computers   \n6      HVAC Principals I, III, Math - Certificate w/MCCC   \n7                                            Cosmetology   \n8                 Diesel Mechanics Technology/Technician   \n...                                                  ...   \n24658                               Six Sigma Black Belt   \n24659              Microsoft Office Specialist Word 2016   \n24660                        Microsoft Office Specialist   \n24661                           A.F.A. Degree: Glass Art   \n24662                     160 Hour CDL-A Training Course   \n24663                              Phlebotomy Technician   \n24664                                     ECG Technician   \n24665                      Young Adult Career Initiative   \n24666                Pre-Employment Transition Services    \n\n                                                  NAME_1  \n0                     Automated Office Systems Processor  \n1      Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...  \n2                Bus.Soft. App./Comp.Office Proc.Medical  \n3                         Microsoft Office Specialist II  \n4                      Computerized Financial Accounting  \n5                              Introduction to Computers  \n6      HVAC Principals I, III, Math - Certificate w/MCCC  \n7                                            Cosmetology  \n8                 Diesel Mechanics Technology/Technician  \n...                                                  ...  \n24658                               Six Sigma Black Belt  \n24659              Microsoft Office Specialist Word 2016  \n24660                        Microsoft Office Specialist  \n24661                           A.F.A. Degree: Glass Art  \n24662                     160 Hour CDL-A Training Course  \n24663                              Phlebotomy Technician  \n24664                                     ECG Technician  \n24665                      Young Adult Career Initiative  \n24666       Pre-Employment Transition Services (Pre-ETS)  \n\n[24667 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STANDARDIZEDNAME_1</th>\n      <th>NAME_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Automated Office Systems Processor</td>\n      <td>Automated Office Systems Processor</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...</td>\n      <td>Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bus.Soft. App./Comp.Office Proc.Medical</td>\n      <td>Bus.Soft. App./Comp.Office Proc.Medical</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Microsoft Office Specialist II</td>\n      <td>Microsoft Office Specialist II</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Computerized Financial Accounting</td>\n      <td>Computerized Financial Accounting</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Introduction to Computers</td>\n      <td>Introduction to Computers</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>HVAC Principals I, III, Math - Certificate w/MCCC</td>\n      <td>HVAC Principals I, III, Math - Certificate w/MCCC</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Cosmetology</td>\n      <td>Cosmetology</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Diesel Mechanics Technology/Technician</td>\n      <td>Diesel Mechanics Technology/Technician</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24658</th>\n      <td>Six Sigma Black Belt</td>\n      <td>Six Sigma Black Belt</td>\n    </tr>\n    <tr>\n      <th>24659</th>\n      <td>Microsoft Office Specialist Word 2016</td>\n      <td>Microsoft Office Specialist Word 2016</td>\n    </tr>\n    <tr>\n      <th>24660</th>\n      <td>Microsoft Office Specialist</td>\n      <td>Microsoft Office Specialist</td>\n    </tr>\n    <tr>\n      <th>24661</th>\n      <td>A.F.A. Degree: Glass Art</td>\n      <td>A.F.A. Degree: Glass Art</td>\n    </tr>\n    <tr>\n      <th>24662</th>\n      <td>160 Hour CDL-A Training Course</td>\n      <td>160 Hour CDL-A Training Course</td>\n    </tr>\n    <tr>\n      <th>24663</th>\n      <td>Phlebotomy Technician</td>\n      <td>Phlebotomy Technician</td>\n    </tr>\n    <tr>\n      <th>24664</th>\n      <td>ECG Technician</td>\n      <td>ECG Technician</td>\n    </tr>\n    <tr>\n      <th>24665</th>\n      <td>Young Adult Career Initiative</td>\n      <td>Young Adult Career Initiative</td>\n    </tr>\n    <tr>\n      <th>24666</th>\n      <td>Pre-Employment Transition Services</td>\n      <td>Pre-Employment Transition Services (Pre-ETS)</td>\n    </tr>\n  </tbody>\n</table>\n<p>24667 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# Now we do a mass search and replace on STANDARDIZED_NAME_1 and STANDRADIZED_NAME with the labels that we have\n",
    "\n",
    "# We follow this overflow thread\n",
    "# see: https://stackoverflow.com/a/48887382/3662899\n",
    "\n",
    "# First, construct an label to abbreviation dictionary\n",
    "label_mapper =\\\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"abbreviation\": \\\n",
    "abbreviations_to_label.abbreviation[: last_labeled_index].values,\n",
    "            \"expanded\": expanded_labels[:last_labeled_index]\n",
    "        }\n",
    "    )\n",
    "copy_over_index = (label_mapper.expanded == \"No Label\") | (label_mapper.expanded == \"Submit.\")\n",
    "label_mapper.expanded[copy_over_index] =\\\n",
    "    label_mapper.abbreviation[copy_over_index]\n",
    "rep_dict = dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "\n",
    "pattern = re.compile(\"|\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "\n",
    "def multiple_replace(string):    \n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)\n",
    "\n",
    "#the_df['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "#    the_df['STANDARDIZEDNAME_1'].map(multiple_replace)\n",
    "\n",
    "#the_df.loc[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'STANDARDIZEDNAME_1']]\n",
    "#the_df[['STANDARDIZEDNAME_1', 'NAME_1']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  CERT_ID                                          CERT_NAME ORG_ID  TRAINING  \\\n0   10249  SAP Certified Technology Associate - System Ad...   0106       NaN   \n1   10052  IBM Certified BPM Developer - WebSphere Lombar...   0814       1.0   \n2   10096  IBM Certified Specialist - Systems Networking ...   0814       0.0   \n\n   EXPERIENCE EITHER  EXAM RENEWAL   CEU REEXAM  ... KEYWORD2 KEYWORD3  \\\n0         NaN   None     1          None   None  ...     None     None   \n1         1.0   None     1          None   None  ...     None     None   \n2         0.0   None     1          None   None  ...     None     None   \n\n  SUPPRESS            DATEADDED COMMENTS VERIFIED UPDATEDBY  \\\n0        1  01/21/2015 16:31:10     None        0        28   \n1        1  10/21/2014 10:43:08     None        0        28   \n2        1  10/24/2014 14:24:09     None        0        28   \n\n                                    CERT_DESCRIPTION DELETED EXAM_DETAILS  \n0  This certification path will validate your cap...       1         None  \n1  This intermediate level certification is inten...       1         None  \n2  The IBM Certified Specialist - System Networki...       1         None  \n\n[3 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CERT_ID</th>\n      <th>CERT_NAME</th>\n      <th>ORG_ID</th>\n      <th>TRAINING</th>\n      <th>EXPERIENCE</th>\n      <th>EITHER</th>\n      <th>EXAM</th>\n      <th>RENEWAL</th>\n      <th>CEU</th>\n      <th>REEXAM</th>\n      <th>...</th>\n      <th>KEYWORD2</th>\n      <th>KEYWORD3</th>\n      <th>SUPPRESS</th>\n      <th>DATEADDED</th>\n      <th>COMMENTS</th>\n      <th>VERIFIED</th>\n      <th>UPDATEDBY</th>\n      <th>CERT_DESCRIPTION</th>\n      <th>DELETED</th>\n      <th>EXAM_DETAILS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10249</td>\n      <td>SAP Certified Technology Associate - System Ad...</td>\n      <td>0106</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n      <td>1</td>\n      <td></td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>01/21/2015 16:31:10</td>\n      <td>None</td>\n      <td>0</td>\n      <td>28</td>\n      <td>This certification path will validate your cap...</td>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10052</td>\n      <td>IBM Certified BPM Developer - WebSphere Lombar...</td>\n      <td>0814</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>None</td>\n      <td>1</td>\n      <td></td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>10/21/2014 10:43:08</td>\n      <td>None</td>\n      <td>0</td>\n      <td>28</td>\n      <td>This intermediate level certification is inten...</td>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10096</td>\n      <td>IBM Certified Specialist - Systems Networking ...</td>\n      <td>0814</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>None</td>\n      <td>1</td>\n      <td></td>\n      <td>None</td>\n      <td>None</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1</td>\n      <td>10/24/2014 14:24:09</td>\n      <td>None</td>\n      <td>0</td>\n      <td>28</td>\n      <td>The IBM Certified Specialist - System Networki...</td>\n      <td>1</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 28 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "\n",
    "# Here we ingest Career One Stop certifications\n",
    "#   I was goign to use this to de-acroymn-ize mentions but now am unsure\n",
    "# if this is critical.\n",
    "\n",
    "# Here we read in a .sql directly as text and parse out the data.\n",
    "# I do this to avoid the need for a database, db drivers, etc. \n",
    "# That said, this represented some investment in constructing the right regexs\n",
    "path = rootpath + externalpath + 'career_one_stop/'\n",
    "credential_sql = 'TEST-2-CERTIFICATIONS.sql' # '2-CERTIFICATIONS.sql'\n",
    "\n",
    "with open(path + credential_sql) as sql:\n",
    "    my_string = sql.read()\n",
    "\n",
    "header_names =\\\n",
    "    (\n",
    "        'CERT_ID', 'CERT_NAME', 'ORG_ID', 'TRAINING', 'EXPERIENCE', \n",
    "        'EITHER', 'EXAM', 'RENEWAL', 'CEU', 'REEXAM', \n",
    "        'CPD', 'CERT_ANY', 'URL', 'ACRONYM', 'NSSB_URL', \n",
    "        'CERT_URL', 'CERT_LAST_UPDATE', 'KEYWORD1', 'KEYWORD2', 'KEYWORD3', \n",
    "        'SUPPRESS', 'DATEADDED', 'COMMENTS', 'VERIFIED', 'UPDATEDBY', \n",
    "        'CERT_DESCRIPTION', 'DELETED', 'EXAM_DETAILS'\n",
    "    )\n",
    "\n",
    "# Pandas assumes atomic python types when reading from records,\n",
    "# See: https://github.com/pandas-dev/pandas/issues/9381, so we need to use\n",
    "# Python types here\n",
    "dtypes =\\\n",
    "    np.dtype(\n",
    "        \"str, str, float, float,\"\n",
    "        \"float, float, float, str,\"\n",
    "        \"float, float, float, float,\"\n",
    "        \"str, str, str, str,\"\n",
    "        \"str, str, str, str,\"\n",
    "        \"str, str, str, str,\" \n",
    "        \"str, str, float, str\"\n",
    "    )\n",
    "\n",
    "flags = re.MULTILINE | re.DOTALL | re.VERBOSE\n",
    "the_fields_regex =\\\n",
    "    \"\"\"\n",
    "    (?P<values>Values\\n\\s+\\()  # Start with the word Value <newline> (\n",
    "        (?P<fields>.*?)        #    Grab all the field content\n",
    "    (?P<end>\\);)               # ... which stops at the terminating paren, ;\n",
    "    \"\"\"\n",
    "\n",
    "the_fields = re.compile(the_fields_regex, flags=flags)\n",
    "\n",
    "a_field_regex =\\\n",
    "    \"\"\"\n",
    "    '(?P<string>.*?)'[,)]           # get a quoted string ending at comma or paran or\n",
    "    |(?P<date_time>TO_DATE\\(.*?\\))  # get the TO_DATE, parse out actual date later or\n",
    "    |(?P<num>\\d),                   # get numeric or\n",
    "    |(?P<null>NULL)                 # get NULL\n",
    "    \"\"\"\n",
    "\n",
    "a_field = re.compile(a_field_regex, flags=flags)\n",
    "\n",
    "require_field_numbers = [1] # should be 13\n",
    "\n",
    "def yield_certification_records(sql_file=my_string, require_field_numbers=require_field_numbers):\n",
    "    # do we skip those w/o certain fields, like acronymns\n",
    "    temp_data = [0]*28\n",
    "    for match in the_fields.finditer(sql_file):\n",
    "        break_match = False\n",
    "\n",
    "        for index, field in enumerate(a_field.finditer( match.group('fields') )):\n",
    "            grp = None\n",
    "            for grp, value in field.groupdict().items():\n",
    "                if value:\n",
    "                    # then we transform the string value into the appropriate type, given the group name\n",
    "                    if grp == 'date_time':\n",
    "                        #  There is a difference between https://regex101.com/r/yphUXY/1/\n",
    "                        # and what I see Python do here; if I don't capture the entire thing\n",
    "                        # it gets re-raised as another potential match, even if I use ?:, etc.\n",
    "                        value = value[9:28] # todo: convert to datetime\n",
    "                    if grp == 'null':\n",
    "                        value = None\n",
    "                        if index in require_field_numbers:\n",
    "                            break_match = True\n",
    "\n",
    "                    if grp == 'num':\n",
    "                        value = int(value)\n",
    "\n",
    "                    temp_data[index] = value\n",
    "                    break # only one possible match value\n",
    "            if break_match: # and don't look at other fields\n",
    "                break\n",
    "\n",
    "        if not break_match:\n",
    "            yield tuple(value for value in temp_data)\n",
    "        \n",
    "        break_match = False\n",
    "\n",
    "certification_df =\\\n",
    "    pd.DataFrame.from_records(\n",
    "        yield_certification_records(),\n",
    "        columns=header_names)\n",
    "certification_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) \n",
    "# Then go after odd static patterns that are common \n",
    "# ... A.A., AAS,e ends-with \"/\", etc etc\n",
    "# \"Applied Certificate in...\" <--- thing is, this could really be a program\n",
    "# the_df.STANDARDIZEDNAME_1 =\\\n",
    "#     the_df.STANDARDIZEDNAME_1.str.replace(\"A.A.\",\"\", case=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)\n",
    "# Now we write out this  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee49f7f4c4664a47a659912addc7119e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "\n",
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "    display(Markdown(row[\"STANDARDIZEDNAME_1\"]))\n",
    "    #display(Markdown(\"**At:** \" + row[\"timestamp\"]))\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x[\"STANDARDIZEDNAME_1\"], y\n",
    "\n",
    "labelling_widget = ClassLabeller(\n",
    "    features=the_df,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "labelling_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.88, 0.88]\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "print_CI(labels=labelling_widget.new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "the_df.to_csv(rootpath + processedpath + \"{}\".format(content_is),\n",
    "              index = False,\n",
    "              chunksize = 10000,\n",
    "              columns=columns_to_save)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}