{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_name_and_name1\"\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "filepath = \"standardized_name.csv\" # builds off of notebook 3 work\n",
    "\n",
    "columns = [\n",
    "    \"NAME_1\",\n",
    "    \"STANDARDIZEDNAME\",\n",
    "    \"NAME\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "columns_to_save = ['STANDARDIZEDNAME_1'] + columns\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "#df = pd.read_excel(rootpath + interimpath + filepath, usecols=columns)\n",
    "df = pd.read_csv(rootpath + interimpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "if not SKIP_THIS:\n",
    "    ONET_TOOLS_TECH_URL_NAME = (\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\", \"onet_tools_tech.csv\")\n",
    "    CAREERONESTOP_CERTIFICATIONS_URL_NAME = (\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\", \"career_one_stop.zip\")\n",
    "\n",
    "    filepath = rootpath + externalpath\n",
    "\n",
    "    for dataset in (ONET_TOOLS_TECH_URL_NAME, CAREERONESTOP_CERTIFICATIONS_URL_NAME):\n",
    "        url, filename = dataset\n",
    "        print(\"running ...\", f'\\nwget -O {filepath+filename} {url}')\n",
    "        os.system(f'wget -O {filepath+filename} {url}')\n",
    "        print(\"filetype is\",  filename[-3:])\n",
    "\n",
    "        if filename[-3:] == 'zip':\n",
    "            with zipfile.ZipFile(filepath+filename,\"r\") as zip_ref:\n",
    "                zipdir = filepath+filename[:-4]\n",
    "                print(\"unzipping {} to ...\".format(filename), zipdir)\n",
    "                os.mkdir(zipdir)\n",
    "                zip_ref.extractall(zipdir)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# A) \n",
    "# The program or course name can start or end with a matching parenthesis. In these cases\n",
    "# we assume that no other matching parenthesis are present and apply \n",
    "# an appropriate regex for that...\n",
    "\n",
    "# First, set up standardized column with default values\n",
    "the_df[\"STANDARDIZEDNAME_1\"] = \"\"\n",
    "\n",
    "# ... then extract names for those with opening parens\n",
    "open_parenthesis_index = the_df.NAME_1.str[0] == '('\n",
    "open_parenthesis_regex = '''\n",
    "                (?P<paren>\\(.*\\)) # get the first parathesis\n",
    "                (?P<the_name>.*)  # then get the actual name\n",
    "                '''\n",
    "\n",
    "the_df.loc[open_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[open_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(open_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then extract names for those with closing parens\n",
    "close_parenthesis_index = the_df.NAME_1.str[-1] == ')'\n",
    "closing_parenthesis_regex = '''\n",
    "                (?P<the_name>.*)  # get the actual name\n",
    "                (?P<paren>\\(.*\\)) # get the last parathensis                \n",
    "                '''\n",
    "the_df.loc[close_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[close_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then we copy over content that has a internal parenthesis with those\n",
    "# parenthesis removed and ignore everything after, e.g. \"ABC (123) DEF\" --> \"ABC\"\n",
    "internal_parenthesis_index =\\\n",
    "    the_df['NAME_1'].str.contains('\\(|\\)', regex=True) &\\\n",
    "        ~(close_parenthesis_index|open_parenthesis_index)\n",
    "\n",
    "the_df.loc[internal_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[internal_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... finally, just copy over everything else\n",
    "no_parenthesis_index = ~(close_parenthesis_index |\\\n",
    "                         open_parenthesis_index  |\\\n",
    "                         internal_parenthesis_index)\n",
    "the_df.loc[no_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[no_parenthesis_index, 'NAME_1']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "constructed abbreviation list...\ndone\n"
    }
   ],
   "source": [
    "# 2)\n",
    "# So now we have silver version data of program, course names\n",
    "# from the cell above, in STANDARDIZEDNAME_1\n",
    "#\n",
    "# To make an incrementally better version we need to expand \n",
    "# abbreviations and acroynmns.\n",
    "\n",
    "# **We do this if we don't skip things and the labeling file does not exist**\n",
    "\n",
    "# Here I identify two pretty common cases of acronymns and abbreviations:\n",
    "#   All caps\n",
    "#   Xx*. <- capitalized inital letter ending with a period\n",
    "\n",
    "# Now let's attempt to extract presumed acronyms and see if we can\n",
    "# directly label them. I assume there are far fewer unique abbreviations\n",
    "# so that a person can actually do this in a short amount of time\n",
    "abbreviation_pickle = rootpath + interimpath + 'abbreviation_label.pickle'\n",
    "\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    flags = re.VERBOSE\n",
    "\n",
    "    #  TODO: check if abbreviation labeled file already exists, if it does\n",
    "    # we skip this portion\n",
    "\n",
    "    # Pandas/Python doesn't like this verbose regex but likes other?\n",
    "    # all_caps_regex = '''\n",
    "    #                 \\b(?P<all_caps>[A-Z]+)  # Get all caps words\n",
    "    #                 [\\s,:\\d]                # sit before a space, comma or digit\n",
    "    #                 '''\n",
    "    all_caps_regex = r'\\b(?P<all_caps>[A-Z]+)[\\s,:\\d]'\n",
    "\n",
    "    dotted_word_regex = r'(?P<dot_abbreviation>[A-Z][a-z]+\\.)'\n",
    "    dotted_word_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<dot_abbreviation>[a-zA-Z][a-z]+\\.)\n",
    "        \"\"\"\n",
    "\n",
    "    the_regexs = \"|\".join([all_caps_regex, dotted_word_regex])\n",
    "\n",
    "    the_abbreviations =\\\n",
    "        the_df['STANDARDIZEDNAME_1'].str\\\n",
    "                                    .extractall(\n",
    "                                        pat=the_regexs,\n",
    "                                        flags=flags)\n",
    "    print('constructed abbreviation list...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created interim abbreviations data frame...\ndone\n"
    }
   ],
   "source": [
    "# Since we've run on the entire dataset we can now\n",
    "# flatten the dataframe, de-duplicate and then directly label\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    len(the_abbreviations.all_caps.unique()) +\\\n",
    "        len(the_abbreviations.dot_abbreviation.unique()) # 1151\n",
    "\n",
    "    # now we need to get the count of unique abbreviations so that we can\n",
    "    # label in priority order. We also drop those abbreviations only occuring once\n",
    "    # since they have a 1 / 26,660 chance of occuring (not worth our effort)\n",
    "\n",
    "    # to properly label the all caps and abbreviations we need the \n",
    "    # context in which they occur. Since we're mapping to one definition\n",
    "    # we assume only the first instance is really needed and label off of that\n",
    "    abbreviations_to_label =\\\n",
    "        pd.concat(\n",
    "            (the_abbreviations.drop_duplicates(\n",
    "                subset=['all_caps'],\n",
    "                keep='first')['all_caps'],\n",
    "            the_abbreviations.drop_duplicates(\n",
    "                subset=['dot_abbreviation'],\n",
    "                keep='first')['dot_abbreviation']\n",
    "            ),\n",
    "            axis=0\n",
    "        ).dropna()\\\n",
    "        .droplevel('match')\\\n",
    "        .reset_index() # so that index is a column\n",
    "\n",
    "    abbreviations_to_label.rename(columns={'index':'the_df_index', 0:'abbreviation'}, inplace=True)\n",
    "    print('created interim abbreviations data frame...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created mapping columns for former labels and their expansions...\n"
    }
   ],
   "source": [
    "if os.path.exists(abbreviation_pickle):\n",
    "    # note here we read the main pickle file and assume \n",
    "    # those pickle files with random extensiosn were/are consolidated into this\n",
    "    # the other pickle files are named to prevent overwriting ongoing work\n",
    "    expanded_labels = pd.read_pickle(abbreviation_pickle)\n",
    "    last_labeled_index = expanded_labels.index(None)\n",
    "    former_labels = abbreviations_to_label.abbreviation[:last_labeled_index] #set(already_labeled[:last_labeled_index])\n",
    "\n",
    "    unseen_abbreviations =\\\n",
    "        abbreviations_to_label.query('abbreviation not in @former_labels')\n",
    "    unseen_abbreviations.abbreviation.value_counts()\n",
    "    # note I'm seeing 1 across the board, both when using not in and in\n",
    "    #   which suggests that we leave these be for now because their occurence\n",
    "    # is so rare out of 26,660, although the bulk may be significant; better\n",
    "    # to circle back though\n",
    "    print('created mapping columns for former labels and their expansions...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done/skipped manual labelling\n"
    }
   ],
   "source": [
    "#  So now we manually label them and dump them here\n",
    "#  This is the procedure we follow\n",
    "#       A) if a capitalized word is an entire word, leave it alone (no label)\n",
    "#       B) provide a label for all dotted abbreviated words\n",
    "if not SKIP_THIS:\n",
    "    def display_func(row):\n",
    "        # Note: We use globally available the_df to get context, bad form I know\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"**Context:** \" +  the_df.loc[ row.the_df_index ].NAME_1 \\\n",
    "            +   \"\\n\\n\" + row.abbreviation\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocessor(x, y):\n",
    "        # only take standardized column, leave everything else\n",
    "        return x.abbreviation, y\n",
    "\n",
    "    if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "        labelling_widget = ClassLabeller(\n",
    "            features=abbreviations_to_label,\n",
    "            model=pipeline,\n",
    "            model_preprocess=preprocessor,\n",
    "            display_func=display_func,\n",
    "            options=['No Label'],\n",
    "            acquisition_function='entropy'\n",
    "        )\n",
    "\n",
    "        labelling_widget\n",
    "print('done/skipped manual labelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Every now and then, with the labels in hand we simply output them (if a file doesn't already exist)\n",
    "# so that we can save them incrementally. We should manually rename older files; this should\n",
    "# be basically a 1 time process.\n",
    "\n",
    "# Temp, save work locally so we don't loooooose it! \n",
    "if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "    import random\n",
    "    random_number = str(random.randint(0,255))\n",
    "    pickle.dump(expanded_labels,\n",
    "                open(abbreviation_pickle+random_number, 'wb'))\n",
    "    print(\"done/don't forget to consolidate abbreviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Business Software.Soft. App/Office Proc.Legal/\n"
    }
   ],
   "source": [
    "# Now we do a mass search and replace on STANDARDIZED_NAME_1 and STANDRADIZED_NAME with the labels that we have\n",
    "\n",
    "# We follow this overflow thread\n",
    "# see: https://stackoverflow.com/a/48887382/3662899\n",
    "\n",
    "# First, construct an abbreviation to its expansion  dictionary\n",
    "label_mapper =\\\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"abbreviation\": \\\n",
    "abbreviations_to_label.abbreviation[:last_labeled_index].values,\n",
    "            \"expanded\": expanded_labels[:last_labeled_index]\n",
    "        }\n",
    "    )\n",
    "copy_over_index = (label_mapper.expanded == \"No Label\") | (label_mapper.expanded == \"Submit.\")\n",
    "label_mapper.expanded[copy_over_index] =\\\n",
    "    label_mapper.abbreviation[copy_over_index]\n",
    "\n",
    "# Add in one-off acronyms observed through labelling that need attention\n",
    "# too do, make this json or a python file\n",
    "one_off_mappings =(\n",
    "    ['AAS', 'Associate of Applied Science'],\n",
    "    ['ESL', 'English as a Second Language'],\n",
    "    ['Bus.Soft.', 'Business Software'],\n",
    "    ['App', 'Application'],\n",
    "    ['App.', 'Application'],\n",
    "    ['Dev.', 'Developer'],\n",
    "    ['CDL', 'Commerical Driver\\'s License']\n",
    ")\n",
    "\n",
    "for mapping in one_off_mappings:\n",
    "    label_mapper.loc[len(label_mapper)+1] = mapping\n",
    "\n",
    "# normally we'd map through rep_dict but we want matches to occur\n",
    "# on word-like boundaries, space, /, and :, we use re.escape to properly escape\n",
    "\n",
    "# Todo: the space regex forces matches at start of string to drop\n",
    "#  need to make a regex that includes ^ starts\n",
    "space = \" \"\n",
    "slash = \"/\"\n",
    "colon = \":\"\n",
    "rep_dict = {\n",
    "    **dict(zip(label_mapper.abbreviation+space, label_mapper.expanded+space)),\n",
    "    **dict(zip(label_mapper.abbreviation+slash, label_mapper.expanded+slash)),\n",
    "    **dict(zip(label_mapper.abbreviation+colon, label_mapper.expanded+colon))\n",
    "}\n",
    "\n",
    "rep_dict = {\n",
    "    **dict(zip(label_mapper.abbreviation, label_mapper.expanded))\n",
    "}\n",
    "\n",
    "\n",
    "#pattern = re.compile(\"[\\b\\W]|\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "pattern = re.compile(\n",
    "    \"|\".join([re.escape(k) for k in rep_dict.keys()]),\n",
    "    re.M)\n",
    "\n",
    "def my_lookup(x):\n",
    "    if not rep_dict.get(x, False):\n",
    "        return rep_dict.get(x[1:], False)\n",
    "\n",
    "start_abbrev = re.compile(\n",
    "    \"^\"+\"|^\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "start_abbrev = re.compile(\n",
    "    \"^HVAC\", re.M)\n",
    "\n",
    "def multiple_replace(string):\n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)\n",
    "\n",
    "def multiple_replace2(string):\n",
    "    return start_abbrev.sub(\n",
    "        lambda x: my_lookup(x), string)\n",
    "\n",
    "def term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    #return left_regex + '(' + re.escape(term) + ')' + right_regex\n",
    "    mystr = left_regex + '(' +\\\n",
    "                f\"?P<{term}>\"   +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' +\\\n",
    "            right_regex\n",
    "    return mystr\n",
    "\n",
    "def term_grouped_regex(term=\"\", right_regex=\"\", left_regex=\"\"):\n",
    "    mystr = left_regex + '(' +\\\n",
    "                re.escape(term) +\\\n",
    "            ')' + right_regex\n",
    "    return mystr\n",
    "\n",
    "\n",
    "def make_grouped_regexes(replace_dict, left_regex=\"\", right_regex=\"\"):\n",
    "    return (make_term_grouped_regex(left_regex=left_regex,\n",
    "                                    term=key,\n",
    "                                    right_regex=right_regex)\\\n",
    "                                        for key in replace_dict.keys()\n",
    "            )\n",
    "\n",
    "# a_abbrev = re.compile(\n",
    "#     \"|\".join([\n",
    "#         make_term_grouped_regex(\n",
    "#             left_regex=\"^\",\n",
    "#             term=k\n",
    "#         ) for k in rep_dict.keys()]), re.M)\n",
    "    # \"|\".join(\n",
    "    #     make_grouped_regexes(rep_dict, left_regex=\"^\")\n",
    "    # ) +\\\n",
    "\n",
    "\n",
    "a_abbrev = re.compile(\n",
    "    \"|\".join(\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'\\s', right_regex=r'\\s')\n",
    "    )\n",
    ")\n",
    "\n",
    "draft_output = the_df.iloc[:100,:][['NAME_1']]\n",
    "pd.set_option('display.max_rows', None)\n",
    "# draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "#     draft_output['NAME_1'].map(multiple_replace3)\n",
    "# draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'NAME_1']]\n",
    "\n",
    "the_content = re.compile(r'\\b(?P<key>\\w+)\\b')\n",
    "\n",
    "test_string = 'Straight Truck Driver - CDL B'\n",
    "test_string = \"Business Software.Soft. App/Office Proc.Legal/\"\n",
    "\n",
    "def lookup_match(matchobj):\n",
    "    #  The match corresponds to a key with regexs \n",
    "    # surrounding it. To properly replace it we\n",
    "    # replace the key with its value in the whole matched\n",
    "    # string\n",
    "    the_original_string = matchobj.group(0)\n",
    "\n",
    "    the_key = the_content.search(\n",
    "        matchobj.group(0)\n",
    "        ).group('key')\n",
    "\n",
    "    the_value = rep_dict.get(the_key, None)\n",
    "    if not the_value:\n",
    "        # try the whole thing\n",
    "        print(rep_dict)\n",
    "        print(the_original_string.strip())\n",
    "        the_value = rep_dict[the_original_string.strip()]\n",
    "        # we then use the entire string\n",
    "        print(the_key, \"|\", the_original_string)\n",
    "\n",
    "    print(the_key, \"|\", the_original_string)\n",
    "    #return the_original_string.replace(lookup_match, the_value)\n",
    "    return the_original_string.replace(the_key, the_value)\n",
    "\n",
    "a_abbrev = re.compile(\n",
    "    \"|\".join(\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'^', right_regex=r'\\s')\n",
    "    ) +\\\n",
    "    \"|\".join(\n",
    "        make_grouped_regexes(rep_dict, left_regex=r'\\s', right_regex=r'\\s')\n",
    "    )\n",
    ")\n",
    "\n",
    "def multiple_replace3(string):\n",
    "    return a_abbrev.sub(lookup_match, string)\n",
    "\n",
    "\n",
    "#re.sub(r'\\s'+'(?P<CDL>CDL)'+r'\\s' + '|' + \"(?P<woot>ABC)\", dashrepl, test_string)\n",
    "#print(a_abbrev)\n",
    "k =multiple_replace3(test_string)\n",
    "print(k)\n",
    "\n",
    "# draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "#     draft_output['NAME_1'].map(multiple_replace3)\n",
    "# draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'NAME_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "> \u001b[0;32m<ipython-input-165-2e0648765658>\u001b[0m(101)\u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[0;32m     99 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmultiple_replace3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m--> 101 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0ma_abbrev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\n<re.Match object; span=(23, 28), match=' CDL '>\n(None, None, None, None, None, None, None, None, None, None, None, None, 'CDL', None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n' CDL '\n' CDL '\n*** TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n{'HVAC': 'Heating, Ventilation and Air Conditioning', 'I': 'I', 'III': 'III', 'MS': 'Microsoft', 'IP': 'IP', 'LAN': 'Local Area Network', 'WAN': 'Wide Area Network', 'MCSE': 'Microsoft Certified Solutions Expert', 'OCP': 'Oracle Certification Program', 'AAS': 'Associate of Applied Science', 'XML': 'XML', 'AA': 'Associate in Arts', 'CDL': \"Commerical Driver's License\", 'B': 'Class B', 'A': 'Class A', 'PC': 'Personal Computer', 'CS': 'CS', 'CAD': 'Computer-aided design', 'II': 'II', 'GED': 'General Education Diploma', 'SMAW': 'Shielded Metal Arc Welding', 'AWS': 'American Welding Society', 'SQL': 'SQL', 'MICROSOFT': 'MICROSOFT', 'OFFICE': 'OFFICE', 'USER': 'USER', 'COMPUTERIZED': 'COMPUTERIZED', 'ELEC': 'Electrical', 'E': 'Electronic', 'ACIS': 'Advanced Computer and Information Security', 'CWSP': 'Certified Wireless Security Professional', 'CWNA': 'Certified Wireless Network Administrator', 'IE': 'IE', 'MEDICAL': 'MEDICAL', 'RECEPTIONIST': 'RECEPTIONIST', 'WITH': 'WITH', 'J': 'J', 'EKG': 'Electrocardiogram', 'WAP': 'Wireless Application Protocol', 'MCSA': 'Microsoft Certified Solution Associate', 'NO': 'NO', 'LONGER': 'LONGER', 'VB': 'Visual Basic', 'ASP': 'ASP', 'ADV': 'Advanced', 'OSHA': 'Occupational Safety and Health Administration', 'CISCO': 'Cisco', 'CCNA': 'Cisco Certified Networking Associate', 'CNC': 'Computer Numeric Controlled', 'CAM': 'Computer-aided manufacturing', 'MOUS': 'Microsoft Office User Specialist', 'W': 'W', 'ESL': 'English as a Second Language', 'RN': 'Registered Nurse', 'YOUTH': 'YOUTH', 'CAREER': 'CAREER', 'IBM': 'IBM', 'DB': 'DB', 'CCIE': 'Cisco Certified Internetwork Expert', 'CEW': 'Certified eBusiness Webmaster', 'CCENT': 'Cisco Certified Network Associate', 'ITIL': 'Information Technology Infrastructure Library', 'DBA': 'Database Administrator', 'C': 'C', 'CCNP': 'Cisco Certified Network Professional', 'R': 'R', 'CRM': 'Customer Relationship Management', 'IIS': 'Internet Information Services', 'S': 'S', 'DOS': 'DOS', 'ALL': 'ALL', 'XP': 'XP', 'MSW': 'MSW', 'COMPUTER': 'COMPUTER', 'APPLICATION': 'APPLICATION', 'MORRIS': ' ', 'SE': 'SE', 'IT': 'Information Technology', 'UNIX': 'UNIX', 'NT': 'NT', 'SPED': 'Special Education', 'O': 'O', 'OT': 'Occupational Therapy', 'G': 'G', 'BJ': 'BJ', 'CR': 'CR', 'V': 'V', 'VI': 'VI', 'ORACLE': 'Oracle', 'SUN': 'Sun', 'SOLARIS': 'Solaris', 'SYSTEM': 'System', 'RESIDENTIAL': 'Residential', 'PROPERTY': 'Property', 'INSPECTION': 'INSPECTION', 'TRAINING': 'TRAINING', 'AAMCO': 'AAMCO', 'CIW': 'Certified Internet Web', 'SWAF': 'Shockwave Adobe Flash', 'HSE': 'High School Equivalent', 'ABE': 'Adult Basic Education', 'D': 'D', 'HIGH': 'HIGH', 'SCHOOL': 'SCHOOL', 'EQUIVALENCY': 'EQUIVALENCY', 'TEST': 'TEST', 'PLC': 'Programmable Logic Controller', 'CPTR': 'CPTR', 'WEB': 'Web', 'DESIGNER': 'DESIGNER', 'NET': 'NET', 'ISO': 'ISO', 'ECE': 'Electrical and Computer Engineering', 'WATA': 'Time Limited', 'WAT': 'Work Adjustment Training', 'ARC': 'Arc', 'ERP': 'Enterprise Resource Planning', 'CPC': 'CPC', 'FAA': 'Federal Aviation Administration', 'MCDBA': 'Microsoft Certified Database Administrator', 'JAVA': 'Java', 'AS': 'Associate in Science', 'READING': 'READING', 'LITERACY': 'LITERACY', 'OCCUPATIONAL': 'OCCUPATIONAL', 'CLASS': 'CLASS', 'PASS': 'PASS', 'BUS': 'BUS', 'MCP': 'Microsoft Certified Professional', 'EXCEL': 'Excel', 'ACCESS': 'Access', 'POWER': 'Power', 'POINT': 'Point', 'FRONTPAGE': 'FRONTPAGE', 'AUTOCAD': 'AUTOCAD', 'MRI': 'Magnetic Resonance Imaging', 'H': 'H', 'HTML': 'HTML', 'WIA': 'Workforce Investment Act', 'AC': 'Air conditioning', 'MFC': 'Microsoft Foundation Class Library', 'AWEP': 'AWEP', 'BOSS': 'BOSS', 'CWNP': 'Certified Wireless Network Professional', 'OST': 'Office Skills Test', 'CPT': 'Current Procedural Terminology', 'EMT': 'Emergency Medical Technician', 'ABL': 'Activity Based Learning', 'PMI': 'Project Management Institute', 'PMP': 'Project Management Professional', 'HHA': 'Home Health Aide', 'MCSD': 'Microsoft Certified Solutions Developer', 'BLS': 'Basic Life Support', 'START': 'START', 'LAS': 'Liberal Arts and Science', 'NJ': 'New Jersey', 'IV': 'IV', 'CCSP': 'Cisco Certified Security Professional', 'AFA': 'Associate in Fine Art', 'ICD': 'ICD', 'ECG': 'ECG', 'WEST': 'WEST', 'ESSEX': 'ESSEX', 'REHAB': 'REHAB', 'WORK': 'WORK', 'ADJUSTMENT': 'ADJUSTMENT', 'HBH': 'Mental and Behavorial Health', 'NOVELL': 'Novell', 'ESOL': 'English for Speakers of Other Languages', 'ICS': 'Industrial and Commerical Specialist', 'ASB': 'Associate Science Business', 'LPN': 'Licensed Practical Nurse', 'MOS': 'Microsoft Office Specialist', 'M': 'M', 'CNE': 'Certified Network Engineer', 'CCEA': 'Citrix Certfied Enterprise Admin', 'SCOPE': 'HighScope', 'CCA': 'Citrix Certified Adminstrator', 'COA': 'Computerized Office Administration', 'NYC': 'New York City', 'HS': 'High School', 'YTTW': 'Youth Transition to Work Program', 'BASIC': 'BASIC', 'NETWORKING': 'NETWORKING', 'QA': 'Quality Assurance', 'TESTING': 'TESTING', 'COMMUNICATION': 'COMMUNICATION', 'SELF': 'SELF', 'CAI': 'Cheryl A. Izzo', 'HPLC': 'High-pressure Liquid Chromatography', 'GC': 'Gas Chromatography', 'UV': 'Ultra-violet', 'IR': 'Infrared', 'HAZWOPER': 'Hazardous Waste and Emergency Response', 'CAT': 'CAT', 'SERVE': 'SERVE', 'TV': 'TV', 'TQM': 'Total Quality Management', 'CIDM': 'Center for Information-Development Management', 'TIG': 'Tungsten Inert Gas', 'OS': 'Operating System', 'X': 'X', 'TOEFL': 'Test of English as a Foreign Language', 'MAC': 'MAC', 'ASE': 'Automotive Service Excellence', 'QC': 'Quality Control', 'CET': 'Civil Engineering Technology', 'CCDA': 'Cisco Certified Design Associate', 'SAS': 'SAS', 'BA': 'BA', 'RCS': 'Residential and Small Commercial Specialist', 'CADD': 'Computer Aided Drafting and Design', 'IC': 'Internet Core Competency', 'JOB': 'JOB', 'SUPPORTED': 'SUPPORTED', 'STW': 'School to Work', 'UMDNJ': 'University of Medicine and Dentistry of New Jersey', 'CP': 'Checkpoint', 'CLOA': 'Certified Legal Office Assistant', 'CBCS': 'Certified Billing and Coding Specialist', 'CMAA': 'Certified Medical Administrative Assistant', 'CDA': 'Certified Dental Assistant', 'RESL': 'Real Estate Salesperson License', 'RECE': 'Real Estate Continuing Education Credits', 'CIWA': 'Certified Internet Webmaster Associate', 'CIWP': 'Certified Internet Webmaster Professional', 'CIWMD': 'Certified Internet Webmaster', 'CMD': 'Certified Macromedia Developer', 'MX': 'MX', 'FMD': 'Flash Macromedia Developer', 'CIS': 'Computer Information System', 'RE': 'Real Estate', 'CCSA': 'Checkpoint Certified Security Administrator', 'ODOSY': 'Out of School Youth Employment', 'PREP': 'PREP', 'ADULT': 'ADULT', 'SKILLS': 'SKILLS', 'SAP': 'SAP', 'DATABASE': 'DATABASE', 'MCITP': 'Microsoft Certified IT Professional', 'SCNP': 'Security Certified Network Professional', 'ACT': 'ACT', 'CNA': 'Cisco Network Associate', 'WIN': 'WIN', 'FLASH': 'FLASH', 'NY': 'New York', 'EZ': 'EZ', 'BUSINESS': 'BUSINESS', 'AUTOMATION': 'AUTOMATION', 'CORE': 'CORE', 'MCAD': 'Microsoft Certified Application Developer', 'CHHA': 'Certified Home and Health Agency', 'UDB': 'IBM Universal Database', 'TASC': 'Test Assessing Secondary Completion', 'DCF': 'Discounted Cash Flow', 'IEPA': 'IEPA', 'MBA': 'Master of Business Adminstration', 'VISUAL': 'VISUAL', 'ADVANCED': 'ADVANCED', 'WINDOWS': 'WINDOWS', 'MCDST': 'Microsoft Certified Desktop Support Technician', 'BS': 'Bachelor of Science', 'SAFETY': 'SAFETY', 'SOP': 'Standard Operating Procedure', 'VIS': 'VIS', 'KF': 'Karl Fischer', 'EXAM': 'EXAM', 'CERTIFIED': 'CERTIFIED', 'BSN': 'Bachelor of Science in Nursing', 'AIDS': 'AIDS', 'SIX': 'SIX', 'NURSING': 'NURSING', 'QMS': 'Quality Management System', 'HR': 'Human Resources', 'CFR': 'Code of Federal Regulations', 'DATA': 'DATA', 'BASE': 'BASE', 'EXPERT': 'EXPERT', 'CFP': 'Certified Financial Planner', 'BSD': 'Berkeley Software Distribution', 'SMT': 'Shielded Metal Arc Welding', 'WORD': 'Word', 'BCSI': 'Building Scalable Cisco Internetworks', 'NETWORK': 'NETWORK', 'CERT': 'Certification', 'PROF': 'Professional', 'SECURITY': 'SECURITY', 'PROFESSIONAL': 'PROFESSIONAL', 'SYSTEMS': 'SYSTEMS', 'ENGINEER': 'ENGINEER', 'WAREHOUSING': 'WAREHOUSING', 'CLINICAL': 'CLINICAL', 'TRIALS': 'TRIALS', 'PACKAGE': 'PACKAGE', 'ASAST': 'ASAST', 'ASNSM': 'ASNSM', 'XSLT': 'XSLT', 'SOAP': 'SOAP', 'CCSE': 'Checkpoint Certified Security Expert', 'CEH': 'Certified Ethical Hacker', 'PIX': 'PIX', 'RBI': 'RBI', 'LM': 'Learning Media', 'HED': 'Health Education', 'Bus.Soft.': 'Business Software', 'App': 'Application', 'App.': 'Application', 'Dev.': 'Developer'}\nre.compile('\\\\s(HVAC)\\\\s|\\\\s(I)\\\\s|\\\\s(III)\\\\s|\\\\s(MS)\\\\s|\\\\s(IP)\\\\s|\\\\s(LAN)\\\\s|\\\\s(WAN)\\\\s|\\\\s(MCSE)\\\\s|\\\\s(OCP)\\\\s|\\\\s(AAS)\\\\s|\\\\s(XML)\\\\s|\\\\s(AA)\\\\s|\\\\s(CDL)\\\\s|\\\\s(B)\\\\s|\\\\s(A)\\\\s|\\\\s(PC)\\\\s|\\\\s(CS)\\\\s|\\\\s()\n"
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "> \u001b[0;32m<ipython-input-47-6921fca2dc44>\u001b[0m(54)\u001b[0;36mmy_lookup\u001b[0;34m()\u001b[0m\n\u001b[0;32m     52 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mmy_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m---> 54 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56 \u001b[0;31mstart_abbrev = re.compile(\n\u001b[0m\n\u001b[1;32m     49 \u001b[0m    \u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50 \u001b[0m    re.M)\n\u001b[1;32m     51 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmy_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56 \u001b[0mstart_abbrev = re.compile(\n\u001b[1;32m     57 \u001b[0m    \u001b[0;34m\"^\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"|^\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58 \u001b[0m    re.M)\n\u001b[1;32m     59 \u001b[0mstart_abbrev = re.compile(\n\n<re.Match object; span=(0, 4), match='HVAC'>\n<re.Match object; span=(0, 4), match='HVAC'> <--\n\u001b[1;32m     60 \u001b[0m    \"^HVAC\", re.M)\n\u001b[1;32m     61 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmultiple_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmultiple_replace2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67 \u001b[0m    return start_abbrev.sub(\n\u001b[1;32m     68 \u001b[0m        lambda x: my_lookup(x), string)\n\u001b[1;32m     69 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[1;32m     71 \u001b[0m\u001b[0mdraft_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72 \u001b[0m\u001b[0;31m#pd.set_option('display.max_rows', False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75 \u001b[0m\u001b[0mdraft_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MULTI_REPLACE_STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76 \u001b[0m    \u001b[0mdraft_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple_replace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78 \u001b[0m\u001b[0mdraft_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MULTI_REPLACE_STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79 \u001b[0m    \u001b[0mdraft_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple_replace2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80 \u001b[0m\u001b[0mdraft_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MULTI_REPLACE_STANDARDIZEDNAME_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NAME_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n"
    }
   ],
   "source": [
    "# regex replace through data\n",
    "\n",
    "draft_output = the_df.dropna(subset=['STANDARDIZEDNAME_1'])\n",
    "\n",
    "draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['STANDARDIZEDNAME_1'].map(multiple_replace)\n",
    "\n",
    "# draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "#     draft_output['STANDARDIZEDNAME_1'].map(multiple_replace2)\n",
    "draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'NAME_1']].iloc[:50,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Here we ingest Career One Stop certifications\n",
    "#   I was goign to use this to de-acroymn-ize mentions but now am unsure\n",
    "# if this is critical. It also may introduce errors, e.g. AES mapping to\n",
    "# the \"wrong acroymn\"\n",
    "\n",
    "# Here we read in a .sql directly as text and parse out the data.\n",
    "# I do this to avoid the need for a database, db drivers, etc. \n",
    "# That said, this represented some investment in constructing the right regexs\n",
    "if not SKIP_THIS:\n",
    "    path = rootpath + externalpath + 'career_one_stop/'\n",
    "    credential_sql = 'TEST-2-CERTIFICATIONS.sql' # '2-CERTIFICATIONS.sql'\n",
    "\n",
    "    with open(path + credential_sql) as sql:\n",
    "        my_string = sql.read()\n",
    "\n",
    "    header_names =\\\n",
    "        (\n",
    "            'CERT_ID', 'CERT_NAME', 'ORG_ID', 'TRAINING', 'EXPERIENCE', \n",
    "            'EITHER', 'EXAM', 'RENEWAL', 'CEU', 'REEXAM', \n",
    "            'CPD', 'CERT_ANY', 'URL', 'ACRONYM', 'NSSB_URL', \n",
    "            'CERT_URL', 'CERT_LAST_UPDATE', 'KEYWORD1', 'KEYWORD2', 'KEYWORD3', \n",
    "            'SUPPRESS', 'DATEADDED', 'COMMENTS', 'VERIFIED', 'UPDATEDBY', \n",
    "            'CERT_DESCRIPTION', 'DELETED', 'EXAM_DETAILS'\n",
    "        )\n",
    "\n",
    "    # Pandas assumes atomic python types when reading from records,\n",
    "    # See: https://github.com/pandas-dev/pandas/issues/9381, so we need to use\n",
    "    # Python types here\n",
    "    dtypes =\\\n",
    "        np.dtype(\n",
    "            \"str, str, float, float,\"\n",
    "            \"float, float, float, str,\"\n",
    "            \"float, float, float, float,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\" \n",
    "            \"str, str, float, str\"\n",
    "        )\n",
    "\n",
    "    flags = re.MULTILINE | re.DOTALL | re.VERBOSE\n",
    "    the_fields_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<values>Values\\n\\s+\\()  # Start with the word Value <newline> (\n",
    "            (?P<fields>.*?)        #    Grab all the field content\n",
    "        (?P<end>\\);)               # ... which stops at the terminating paren, ;\n",
    "        \"\"\"\n",
    "\n",
    "    the_fields = re.compile(the_fields_regex, flags=flags)\n",
    "\n",
    "    a_field_regex =\\\n",
    "        \"\"\"\n",
    "        '(?P<string>.*?)'[,)]           # get a quoted string ending at comma or paran or\n",
    "        |(?P<date_time>TO_DATE\\(.*?\\))  # get the TO_DATE, parse out actual date later or\n",
    "        |(?P<num>\\d),                   # get numeric or\n",
    "        |(?P<null>NULL)                 # get NULL\n",
    "        \"\"\"\n",
    "\n",
    "    a_field = re.compile(a_field_regex, flags=flags)\n",
    "\n",
    "    require_field_numbers = [1] # should be 13\n",
    "\n",
    "    def yield_certification_records(sql_file=my_string, require_field_numbers=require_field_numbers):\n",
    "        # do we skip those w/o certain fields, like acronymns\n",
    "        temp_data = [0]*28\n",
    "        for match in the_fields.finditer(sql_file):\n",
    "            break_match = False\n",
    "\n",
    "            for index, field in enumerate(a_field.finditer( match.group('fields') )):\n",
    "                grp = None\n",
    "                for grp, value in field.groupdict().items():\n",
    "                    if value:\n",
    "                        # then we transform the string value into the appropriate type, given the group name\n",
    "                        if grp == 'date_time':\n",
    "                            #  There is a difference between https://regex101.com/r/yphUXY/1/\n",
    "                            # and what I see Python do here; if I don't capture the entire thing\n",
    "                            # it gets re-raised as another potential match, even if I use ?:, etc.\n",
    "                            value = value[9:28] # todo: convert to datetime\n",
    "                        if grp == 'null':\n",
    "                            value = None\n",
    "                            if index in require_field_numbers:\n",
    "                                break_match = True\n",
    "\n",
    "                        if grp == 'num':\n",
    "                            value = int(value)\n",
    "\n",
    "                        temp_data[index] = value\n",
    "                        break # only one possible match value\n",
    "                if break_match: # and don't look at other fields\n",
    "                    break\n",
    "\n",
    "            if not break_match:\n",
    "                yield tuple(value for value in temp_data)\n",
    "            \n",
    "            break_match = False\n",
    "\n",
    "    certification_df =\\\n",
    "        pd.DataFrame.from_records(\n",
    "            yield_certification_records(),\n",
    "            columns=header_names)\n",
    "    certification_df\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) \n",
    "# Then go after odd static patterns that are common \n",
    "# ... A.A., AAS,e ends-with \"/\", etc etc\n",
    "# \"Applied Certificate in...\" <--- thing is, this could really be a program\n",
    "# the_df.STANDARDIZEDNAME_1 =\\\n",
    "#     the_df.STANDARDIZEDNAME_1.str.replace(\"A.A.\",\"\", case=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "# along with the provider name. My goal is to have 85%+ standardized, send out\n",
    "# that 85% will come from the jefferey's interval\n",
    "\n",
    "# Evaluation Rubric:\n",
    "#   A) Here we label clearly wrong snippets, anything that is marginal we mark as\n",
    "# standardized for purposes of this evaluation because we want to err on the side\n",
    "# of giving overly specific information, which includes odd info\n",
    "#   B) We also click through quickly, not overly dwelling one any one example, the\n",
    "# goal here is to get the evaulation done quickly since it's so manual\n",
    "#   C) For now we ignore casingl there does need to be a camel casing applied to\n",
    "# all caps\n",
    "\n",
    "# We create a series of data to evaluate\n",
    "columns_to_check = ['MULTI_REPLACE_STANDARDIZEDNAME_1', 'STANDARDIZEDNAME']\n",
    "the_data =\\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            draft_output[columns_to_check[0]].to_numpy(),\n",
    "            the_df[columns_to_check[1]].to_numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# we shuffle the data to elminate any bias across/within the columns when\n",
    "# evaluting\n",
    "random.Random(42).shuffle(the_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a37b5fe2f96a4f35a9c91cca25d2d6ca"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "    display(Markdown(row))\n",
    "    #display(Markdown(\"**At:** \" + row[\"timestamp\"]))\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x, y\n",
    "\n",
    "verification_widget = ClassLabeller(\n",
    "    features=the_data,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "verification_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.99, 0.99]\nWe examined 102 labels, of which 101 are correct.\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "    print(f\"We examined {len(labels) - not_examined_count} labels, of which {successful_count} are correct.\")\n",
    "print_CI(labels=verification_widget.new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "write_out = the_df\n",
    "\n",
    "write_out['STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1']\n",
    "\n",
    "# shuffe the rows to better remove temporal baises\n",
    "write_out =\\\n",
    "    the_df.sample(frac=1, random_state=42, axis=0).reset_index(drop=True)\n",
    "\n",
    "write_out.to_csv(rootpath + interimpath + content_is + \".csv\",\n",
    "                index = False,\n",
    "                chunksize = 10000,\n",
    "                columns=columns_to_save)\n",
    "\n",
    "write_out.to_excel(rootpath + processedpath + content_is + \".xls\",\n",
    "            sheet_name=\"Standardized NAME and NAME_1\",\n",
    "            index=False,\n",
    "            columns=columns_to_save)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}