{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "# active labeler related\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import ComplementNB  # corrects for class imbalance, SGD is pretty good too\n",
    "from sklearn.pipeline import Pipeline\n",
    "from superintendent import ClassLabeller\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', ComplementNB()),\n",
    "])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Set up columns to keep, fields, locations for writing\n",
    "rootpath = \"/hdd/work/d4ad_standardization/\"\n",
    "processedpath = \"D4AD_Standardization/data/processed/\"\n",
    "externalpath = \"D4AD_Standardization/data/external/\"\n",
    "interimpath = \"D4AD_Standardization/data/interim/\"\n",
    "\n",
    "content_is = \"standardized_name1.csv\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "\n",
    "filepath = \"./D4AD_Standardization/data/raw/etpl_all_programsJune3.xls\"\n",
    "filepath = \"standardized_name.csv\" # builds off of notebook 3 work\n",
    "\n",
    "columns = [\n",
    "    \"STANDARDIZEDNAME\",\n",
    "    \"NAME\",\n",
    "    \"NAME_1\",\n",
    "    \"DESCRIPTION\",\n",
    "    \"PREREQUISITES\",\n",
    "    \"FEATURESDESCRIPTION\",\n",
    "    \"STREET1\",\n",
    "    \"CITY\",\n",
    "    \"STATE\",\n",
    "    \"ZIP\",\n",
    "    \"WEBSITE\",\n",
    "    \"COUNTY\",\n",
    "    \"NONGOVAPPROVAL\",\n",
    "    \"STATECOMMENTS\",\n",
    "    \"CIPCODE\",\n",
    "    \"PROVIDERID\",\n",
    "    \"APPROVINGAGENCYID\"\n",
    "]\n",
    "\n",
    "columns_to_save = ['STANDARDIZEDNAME_1'] + columns\n",
    "\n",
    "SKIP_THIS = True # helps me be able to run all and not worry about pulling things\n",
    "# I already know I have on disk\n",
    "\n",
    "#df = pd.read_excel(rootpath + interimpath + filepath, usecols=columns)\n",
    "df = pd.read_csv(rootpath + interimpath + filepath, usecols=columns)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "if not SKIP_THIS:\n",
    "    ONET_TOOLS_TECH_URL_NAME = (\"https://www.onetcenter.org/dl_files/database/db_20_1_text/Tools%20and%20Technology.txt\", \"onet_tools_tech.csv\")\n",
    "    CAREERONESTOP_CERTIFICATIONS_URL_NAME = (\"https://www.careeronestop.org/TridionMultimedia/tcm24-48614_CareerOnestop_Certifications_07072020.zip\", \"career_one_stop.zip\")\n",
    "\n",
    "    filepath = rootpath + externalpath\n",
    "\n",
    "    for dataset in (ONET_TOOLS_TECH_URL_NAME, CAREERONESTOP_CERTIFICATIONS_URL_NAME):\n",
    "        url, filename = dataset\n",
    "        print(\"running ...\", f'\\nwget -O {filepath+filename} {url}')\n",
    "        os.system(f'wget -O {filepath+filename} {url}')\n",
    "        print(\"filetype is\",  filename[-3:])\n",
    "\n",
    "        if filename[-3:] == 'zip':\n",
    "            with zipfile.ZipFile(filepath+filename,\"r\") as zip_ref:\n",
    "                zipdir = filepath+filename[:-4]\n",
    "                print(\"unzipping {} to ...\".format(filename), zipdir)\n",
    "                os.mkdir(zipdir)\n",
    "                zip_ref.extractall(zipdir)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', False)\n",
    "\n",
    "the_df = df #df.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# A) \n",
    "# The program or course name can start or end with a matching parenthesis. In these cases\n",
    "# we assume that no other matching parenthesis are present and apply \n",
    "# an appropriate regex for that...\n",
    "\n",
    "# First, set up standardized column with default values\n",
    "the_df[\"STANDARDIZEDNAME_1\"] = \"\"\n",
    "\n",
    "# ... then extract names for those with opening parens\n",
    "open_parenthesis_index = the_df.NAME_1.str[0] == '('\n",
    "open_parenthesis_regex = '''\n",
    "                (?P<paren>\\(.*\\)) # get the first parathesis\n",
    "                (?P<the_name>.*)  # then get the actual name\n",
    "                '''\n",
    "\n",
    "the_df.loc[open_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[open_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(open_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then extract names for those with closing parens\n",
    "close_parenthesis_index = the_df.NAME_1.str[-1] == ')'\n",
    "closing_parenthesis_regex = '''\n",
    "                (?P<the_name>.*)  # get the actual name\n",
    "                (?P<paren>\\(.*\\)) # get the last parathensis                \n",
    "                '''\n",
    "the_df.loc[close_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[close_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... then we copy over content that has a internal parenthesis with those\n",
    "# parenthesis removed and ignore everything after, e.g. \"ABC (123) DEF\" --> \"ABC\"\n",
    "internal_parenthesis_index =\\\n",
    "    the_df['NAME_1'].str.contains('\\(|\\)', regex=True) &\\\n",
    "        ~(close_parenthesis_index|open_parenthesis_index)\n",
    "\n",
    "the_df.loc[internal_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[internal_parenthesis_index, 'NAME_1']\\\n",
    "          .str\\\n",
    "          .extract(closing_parenthesis_regex, flags=re.VERBOSE).the_name\n",
    "\n",
    "# ... finally, just copy over everything else\n",
    "no_parenthesis_index = ~(close_parenthesis_index |\\\n",
    "                         open_parenthesis_index  |\\\n",
    "                         internal_parenthesis_index)\n",
    "the_df.loc[no_parenthesis_index, \"STANDARDIZEDNAME_1\"] =\\\n",
    "    the_df.loc[no_parenthesis_index, 'NAME_1']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "constructed abbreviation list...\ndone\n"
    }
   ],
   "source": [
    "# 2)\n",
    "# So now we have silver version data of program, course names\n",
    "# from the cell above, in STANDARDIZEDNAME_1\n",
    "#\n",
    "# To make an incrementally better version we need to expand \n",
    "# abbreviations and acroynmns.\n",
    "\n",
    "# **We do this if we don't skip things and the labeling file does not exist**\n",
    "\n",
    "# Here I identify two pretty common cases of acronymns and abbreviations:\n",
    "#   All caps\n",
    "#   Xx*. <- capitalized inital letter ending with a period\n",
    "\n",
    "# Now let's attempt to extract presumed acronyms and see if we can\n",
    "# directly label them. I assume there are far fewer unique abbreviations\n",
    "# so that a person can actually do this in a short amount of time\n",
    "abbreviation_pickle = rootpath + interimpath + 'abbreviation_label.pickle'\n",
    "\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    flags = re.VERBOSE\n",
    "\n",
    "    #  TODO: check if abbreviation labeled file already exists, if it does\n",
    "    # we skip this portion\n",
    "\n",
    "    # Pandas/Python doesn't like this verbose regex but likes other?\n",
    "    # all_caps_regex = '''\n",
    "    #                 \\b(?P<all_caps>[A-Z]+)  # Get all caps words\n",
    "    #                 [\\s,:\\d]                # sit before a space, comma or digit\n",
    "    #                 '''\n",
    "    all_caps_regex = r'\\b(?P<all_caps>[A-Z]+)[\\s,:\\d]'\n",
    "\n",
    "    dotted_word_regex = r'(?P<dot_abbreviation>[A-Z][a-z]+\\.)'\n",
    "    dotted_word_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<dot_abbreviation>[a-zA-Z][a-z]+\\.)\n",
    "        \"\"\"\n",
    "\n",
    "    the_regexs = \"|\".join([all_caps_regex, dotted_word_regex])\n",
    "\n",
    "    the_abbreviations =\\\n",
    "        the_df['STANDARDIZEDNAME_1'].str\\\n",
    "                                    .extractall(\n",
    "                                        pat=the_regexs,\n",
    "                                        flags=flags)\n",
    "    print('constructed abbreviation list...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created abbreviations data frame...\ndone\n"
    }
   ],
   "source": [
    "# Since we've run on the entire dataset we can now\n",
    "# flatten the dataframe, de-duplicate and then directly label\n",
    "if os.path.exists(abbreviation_pickle):\n",
    "    len(the_abbreviations.all_caps.unique()) +\\\n",
    "        len(the_abbreviations.dot_abbreviation.unique()) # 1151\n",
    "\n",
    "    # now we need to get the count of unique abbreviations so that we can\n",
    "    # label in priority order. We also drop those abbreviations only occuring once\n",
    "    # since they have a 1 / 26,660 chance of occuring (not worth our effort)\n",
    "\n",
    "    # to properly label the all caps and abbreviations we need the \n",
    "    # context in which they occur. Since we're mapping to one definition\n",
    "    # we assume only the first instance is really needed and label off of that\n",
    "    abbreviations_to_label =\\\n",
    "        pd.concat(\n",
    "            (the_abbreviations.drop_duplicates(\n",
    "                subset=['all_caps'],\n",
    "                keep='first')['all_caps'],\n",
    "            the_abbreviations.drop_duplicates(\n",
    "                subset=['dot_abbreviation'],\n",
    "                keep='first')['dot_abbreviation']\n",
    "            ),\n",
    "            axis=0\n",
    "        ).dropna()\\\n",
    "        .droplevel('match')\\\n",
    "        .reset_index() # so that index is a column\n",
    "\n",
    "    abbreviations_to_label.rename(columns={'index':'the_df_index', 0:'abbreviation'}, inplace=True)\n",
    "    print('created interim abbreviations data frame...')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "created mapping columns for former labels and their expansions...\n"
    }
   ],
   "source": [
    "if os.path.exists(abbreviation_pickle):\n",
    "    # note here we read the main pickle file and assume \n",
    "    # those pickle files with random extensiosn were/are consolidated into this\n",
    "    # the other pickle files are named to prevent overwriting ongoing work\n",
    "    expanded_labels = pd.read_pickle(abbreviation_pickle)\n",
    "    last_labeled_index = expanded_labels.index(None)\n",
    "    former_labels = abbreviations_to_label.abbreviation[:last_labeled_index] #set(already_labeled[:last_labeled_index])\n",
    "\n",
    "    unseen_abbreviations =\\\n",
    "        abbreviations_to_label.query('abbreviation not in @former_labels')\n",
    "    unseen_abbreviations.abbreviation.value_counts()\n",
    "    # note I'm seeing 1 across the board, both when using not in and in\n",
    "    #   which suggests that we leave these be for now because their occurence\n",
    "    # is so rare out of 26,660, although the bulk may be significant; better\n",
    "    # to circle back though\n",
    "    print('created mapping columns for former labels and their expansions...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done/skipped manual labelling\n"
    }
   ],
   "source": [
    "#  So now we manually label them and dump them here\n",
    "#  This is the procedure we follow\n",
    "#       A) if a capitalized word is an entire word, leave it alone (no label)\n",
    "#       B) provide a label for all dotted abbreviated words\n",
    "if SKIP_THIS:\n",
    "    def display_func(row):\n",
    "        # Note: We use globally available the_df to get context, bad form I know\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"**Context:** \" +  the_df.loc[ row.the_df_index ].NAME_1 \\\n",
    "            +   \"\\n\\n\" + row.abbreviation\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def preprocessor(x, y):\n",
    "        # only take standardized column, leave everything else\n",
    "        return x.abbreviation, y\n",
    "\n",
    "    if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "        labelling_widget = ClassLabeller(\n",
    "            features=abbreviations_to_label,\n",
    "            model=pipeline,\n",
    "            model_preprocess=preprocessor,\n",
    "            display_func=display_func,\n",
    "            options=['No Label'],\n",
    "            acquisition_function='entropy'\n",
    "        )\n",
    "\n",
    "        labelling_widget\n",
    "print('done/skipped manual labelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Every now and then, with the labels in hand we simply output them (if a file doesn't already exist)\n",
    "# so that we can save them incrementally. We should manually rename older files; this should\n",
    "# be basically a 1 time process.\n",
    "\n",
    "# Temp, save work locally so we don't loooooose it! \n",
    "if os.path.exists(abbreviation_pickle) and not SKIP_THIS:\n",
    "    import random\n",
    "    random_number = str(random.randint(0,255))\n",
    "    pickle.dump(expanded_labels,\n",
    "                open(abbreviation_pickle+random_number, 'wb'))\n",
    "    print(\"done/don't forget to consolidate abbreviations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                        MULTI_REPLACE_STANDARDIZEDNAME_1  \\\n0                     Automated Office Systems Processor   \n1      Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...   \n2                Bus.Soft. App./Comp.Office Proc.Medical   \n3                         Microsoft Office Specialist II   \n4                      Computerized Financial Accounting   \n5                              Introduction to Computers   \n6      Heating, Ventilation and Air Conditioning Prin...   \n7                                            Cosmetology   \n8                 Diesel Mechanics Technology/Technician   \n...                                                  ...   \n24658                               Six Sigma Black Belt   \n24659              Microsoft Office Specialist Word 2016   \n24660                        Microsoft Office Specialist   \n24661                           A.F.A. Degree: Glass Art   \n24662               160 Hour CDL-Class A Training Course   \n24663                              Phlebotomy Technician   \n24664                                     ECG Technician   \n24665                      Young Adult Career Initiative   \n24666                Pre-Employment Transition Services    \n\n                                      STANDARDIZEDNAME_1  \n0                     Automated Office Systems Processor  \n1      Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...  \n2                Bus.Soft. App./Comp.Office Proc.Medical  \n3                         Microsoft Office Specialist II  \n4                      Computerized Financial Accounting  \n5                              Introduction to Computers  \n6      HVAC Principals I, III, Math - Certificate w/MCCC  \n7                                            Cosmetology  \n8                 Diesel Mechanics Technology/Technician  \n...                                                  ...  \n24658                               Six Sigma Black Belt  \n24659              Microsoft Office Specialist Word 2016  \n24660                        Microsoft Office Specialist  \n24661                           A.F.A. Degree: Glass Art  \n24662                     160 Hour CDL-A Training Course  \n24663                              Phlebotomy Technician  \n24664                                     ECG Technician  \n24665                      Young Adult Career Initiative  \n24666                Pre-Employment Transition Services   \n\n[24620 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MULTI_REPLACE_STANDARDIZEDNAME_1</th>\n      <th>STANDARDIZEDNAME_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Automated Office Systems Processor</td>\n      <td>Automated Office Systems Processor</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...</td>\n      <td>Bus.Soft. App/Office Proc.Legal/Mach.Transcrip...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Bus.Soft. App./Comp.Office Proc.Medical</td>\n      <td>Bus.Soft. App./Comp.Office Proc.Medical</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Microsoft Office Specialist II</td>\n      <td>Microsoft Office Specialist II</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Computerized Financial Accounting</td>\n      <td>Computerized Financial Accounting</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Introduction to Computers</td>\n      <td>Introduction to Computers</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Heating, Ventilation and Air Conditioning Prin...</td>\n      <td>HVAC Principals I, III, Math - Certificate w/MCCC</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Cosmetology</td>\n      <td>Cosmetology</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Diesel Mechanics Technology/Technician</td>\n      <td>Diesel Mechanics Technology/Technician</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24658</th>\n      <td>Six Sigma Black Belt</td>\n      <td>Six Sigma Black Belt</td>\n    </tr>\n    <tr>\n      <th>24659</th>\n      <td>Microsoft Office Specialist Word 2016</td>\n      <td>Microsoft Office Specialist Word 2016</td>\n    </tr>\n    <tr>\n      <th>24660</th>\n      <td>Microsoft Office Specialist</td>\n      <td>Microsoft Office Specialist</td>\n    </tr>\n    <tr>\n      <th>24661</th>\n      <td>A.F.A. Degree: Glass Art</td>\n      <td>A.F.A. Degree: Glass Art</td>\n    </tr>\n    <tr>\n      <th>24662</th>\n      <td>160 Hour CDL-Class A Training Course</td>\n      <td>160 Hour CDL-A Training Course</td>\n    </tr>\n    <tr>\n      <th>24663</th>\n      <td>Phlebotomy Technician</td>\n      <td>Phlebotomy Technician</td>\n    </tr>\n    <tr>\n      <th>24664</th>\n      <td>ECG Technician</td>\n      <td>ECG Technician</td>\n    </tr>\n    <tr>\n      <th>24665</th>\n      <td>Young Adult Career Initiative</td>\n      <td>Young Adult Career Initiative</td>\n    </tr>\n    <tr>\n      <th>24666</th>\n      <td>Pre-Employment Transition Services</td>\n      <td>Pre-Employment Transition Services</td>\n    </tr>\n  </tbody>\n</table>\n<p>24620 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Now we do a mass search and replace on STANDARDIZED_NAME_1 and STANDRADIZED_NAME with the labels that we have\n",
    "\n",
    "# We follow this overflow thread\n",
    "# see: https://stackoverflow.com/a/48887382/3662899\n",
    "\n",
    "# First, construct an abbreviation to its expansion  dictionary\n",
    "label_mapper =\\\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"abbreviation\": \\\n",
    "abbreviations_to_label.abbreviation[:last_labeled_index].values,\n",
    "            \"expanded\": expanded_labels[:last_labeled_index]\n",
    "        }\n",
    "    )\n",
    "copy_over_index = (label_mapper.expanded == \"No Label\") | (label_mapper.expanded == \"Submit.\")\n",
    "label_mapper.expanded[copy_over_index] =\\\n",
    "    label_mapper.abbreviation[copy_over_index]\n",
    "\n",
    "# normally we'd map through rep_dict but we want matches to occur\n",
    "# on word-like boundaries, space, /, and :, we use re.escape to properly escape\n",
    "\n",
    "# Todo: the space regex forces matches at start of string to drop\n",
    "#  need to make a regex that includes ^ starts\n",
    "space = \" \"\n",
    "slash = \"/\"\n",
    "colon = \":\"\n",
    "rep_dict = {\n",
    "    **dict(zip(label_mapper.abbreviation+space, label_mapper.expanded+space)),\n",
    "    **dict(zip(label_mapper.abbreviation+slash, label_mapper.expanded+slash)),\n",
    "    **dict(zip(label_mapper.abbreviation+colon, label_mapper.expanded+colon))\n",
    "}\n",
    "\n",
    "#pattern = re.compile(\"[\\b\\W]|\".join([re.escape(k) for k in rep_dict.keys()]), re.M)\n",
    "pattern = re.compile(\n",
    "    \"|\".join([re.escape(k) for k in rep_dict.keys()])\n",
    "    re.M)\n",
    "\n",
    "def multiple_replace(string):\n",
    "    return pattern.sub(lambda x: rep_dict[x.group(0)], string)\n",
    "\n",
    "draft_output = the_df.dropna(subset=['STANDARDIZEDNAME_1'])\n",
    "\n",
    "draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['STANDARDIZEDNAME_1'].map(multiple_replace)\n",
    "draft_output[['MULTI_REPLACE_STANDARDIZEDNAME_1', 'STANDARDIZEDNAME_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "done\n"
    }
   ],
   "source": [
    "# Here we ingest Career One Stop certifications\n",
    "#   I was goign to use this to de-acroymn-ize mentions but now am unsure\n",
    "# if this is critical. It also may introduce errors, e.g. AES mapping to\n",
    "# the \"wrong acroymn\"\n",
    "\n",
    "# Here we read in a .sql directly as text and parse out the data.\n",
    "# I do this to avoid the need for a database, db drivers, etc. \n",
    "# That said, this represented some investment in constructing the right regexs\n",
    "if SKIP_THIS:\n",
    "    path = rootpath + externalpath + 'career_one_stop/'\n",
    "    credential_sql = 'TEST-2-CERTIFICATIONS.sql' # '2-CERTIFICATIONS.sql'\n",
    "\n",
    "    with open(path + credential_sql) as sql:\n",
    "        my_string = sql.read()\n",
    "\n",
    "    header_names =\\\n",
    "        (\n",
    "            'CERT_ID', 'CERT_NAME', 'ORG_ID', 'TRAINING', 'EXPERIENCE', \n",
    "            'EITHER', 'EXAM', 'RENEWAL', 'CEU', 'REEXAM', \n",
    "            'CPD', 'CERT_ANY', 'URL', 'ACRONYM', 'NSSB_URL', \n",
    "            'CERT_URL', 'CERT_LAST_UPDATE', 'KEYWORD1', 'KEYWORD2', 'KEYWORD3', \n",
    "            'SUPPRESS', 'DATEADDED', 'COMMENTS', 'VERIFIED', 'UPDATEDBY', \n",
    "            'CERT_DESCRIPTION', 'DELETED', 'EXAM_DETAILS'\n",
    "        )\n",
    "\n",
    "    # Pandas assumes atomic python types when reading from records,\n",
    "    # See: https://github.com/pandas-dev/pandas/issues/9381, so we need to use\n",
    "    # Python types here\n",
    "    dtypes =\\\n",
    "        np.dtype(\n",
    "            \"str, str, float, float,\"\n",
    "            \"float, float, float, str,\"\n",
    "            \"float, float, float, float,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\"\n",
    "            \"str, str, str, str,\" \n",
    "            \"str, str, float, str\"\n",
    "        )\n",
    "\n",
    "    flags = re.MULTILINE | re.DOTALL | re.VERBOSE\n",
    "    the_fields_regex =\\\n",
    "        \"\"\"\n",
    "        (?P<values>Values\\n\\s+\\()  # Start with the word Value <newline> (\n",
    "            (?P<fields>.*?)        #    Grab all the field content\n",
    "        (?P<end>\\);)               # ... which stops at the terminating paren, ;\n",
    "        \"\"\"\n",
    "\n",
    "    the_fields = re.compile(the_fields_regex, flags=flags)\n",
    "\n",
    "    a_field_regex =\\\n",
    "        \"\"\"\n",
    "        '(?P<string>.*?)'[,)]           # get a quoted string ending at comma or paran or\n",
    "        |(?P<date_time>TO_DATE\\(.*?\\))  # get the TO_DATE, parse out actual date later or\n",
    "        |(?P<num>\\d),                   # get numeric or\n",
    "        |(?P<null>NULL)                 # get NULL\n",
    "        \"\"\"\n",
    "\n",
    "    a_field = re.compile(a_field_regex, flags=flags)\n",
    "\n",
    "    require_field_numbers = [1] # should be 13\n",
    "\n",
    "    def yield_certification_records(sql_file=my_string, require_field_numbers=require_field_numbers):\n",
    "        # do we skip those w/o certain fields, like acronymns\n",
    "        temp_data = [0]*28\n",
    "        for match in the_fields.finditer(sql_file):\n",
    "            break_match = False\n",
    "\n",
    "            for index, field in enumerate(a_field.finditer( match.group('fields') )):\n",
    "                grp = None\n",
    "                for grp, value in field.groupdict().items():\n",
    "                    if value:\n",
    "                        # then we transform the string value into the appropriate type, given the group name\n",
    "                        if grp == 'date_time':\n",
    "                            #  There is a difference between https://regex101.com/r/yphUXY/1/\n",
    "                            # and what I see Python do here; if I don't capture the entire thing\n",
    "                            # it gets re-raised as another potential match, even if I use ?:, etc.\n",
    "                            value = value[9:28] # todo: convert to datetime\n",
    "                        if grp == 'null':\n",
    "                            value = None\n",
    "                            if index in require_field_numbers:\n",
    "                                break_match = True\n",
    "\n",
    "                        if grp == 'num':\n",
    "                            value = int(value)\n",
    "\n",
    "                        temp_data[index] = value\n",
    "                        break # only one possible match value\n",
    "                if break_match: # and don't look at other fields\n",
    "                    break\n",
    "\n",
    "            if not break_match:\n",
    "                yield tuple(value for value in temp_data)\n",
    "            \n",
    "            break_match = False\n",
    "\n",
    "    certification_df =\\\n",
    "        pd.DataFrame.from_records(\n",
    "            yield_certification_records(),\n",
    "            columns=header_names)\n",
    "    certification_df\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) \n",
    "# Then go after odd static patterns that are common \n",
    "# ... A.A., AAS,e ends-with \"/\", etc etc\n",
    "# \"Applied Certificate in...\" <--- thing is, this could really be a program\n",
    "# the_df.STANDARDIZEDNAME_1 =\\\n",
    "#     the_df.STANDARDIZEDNAME_1.str.replace(\"A.A.\",\"\", case=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the evaluation part of the program and course name standardizations\n",
    "# along with the provider name. My goal is to have 85%+ standardized, send out\n",
    "# that 85% will come from the jefferey's interval\n",
    "\n",
    "# Evaluation Rubric:\n",
    "#   A) Here we label clearly wrong snippets, anything that is marginal we mark as\n",
    "# standardized for purposes of this evaluation because we want to err on the side\n",
    "# of giving overly specific information, which includes odd info\n",
    "#   B) We also click through quickly, not overly dwelling one any one example, the\n",
    "# goal here is to get the evaulation done quickly since it's so manual\n",
    "#   C) For now we ignore casingl there does need to be a camel casing applied to\n",
    "# all caps\n",
    "\n",
    "# We create a series of data to evaluate\n",
    "columns_to_check = ['MULTI_REPLACE_STANDARDIZEDNAME_1', 'STANDARDIZEDNAME']\n",
    "the_data =\\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            draft_output[columns_to_check[0]].to_numpy(),\n",
    "            the_df[columns_to_check[1]].to_numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# we shuffle the data to elminate any bias across/within the columns when\n",
    "# evaluting\n",
    "random.Random(42).shuffle(the_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "ClassLabeller(children=(HBox(children=(HBox(children=(FloatProgress(value=0.0, description='Progress:', max=1.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a37b5fe2f96a4f35a9c91cca25d2d6ca"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def display_func(row):\n",
    "    \"\"\"\n",
    "    The display function gets passed your data - in the\n",
    "    case of a dataframe, it gets passed a row - and then\n",
    "    has to \"display\" your data in whatever way you want.\n",
    "\n",
    "    It doesn't need to return anything\n",
    "    \"\"\"\n",
    "    display(Markdown(row))\n",
    "    #display(Markdown(\"**At:** \" + row[\"timestamp\"]))\n",
    "\n",
    "def preprocessor(x, y):\n",
    "    # only take standardized column, leave everything else\n",
    "    return x, y\n",
    "\n",
    "verification_widget = ClassLabeller(\n",
    "    features=the_data,\n",
    "    model=pipeline,\n",
    "    model_preprocess=preprocessor,\n",
    "    display_func=display_func,\n",
    "    options=['standardized', 'not standardized'],\n",
    "    acquisition_function='margin'\n",
    ")\n",
    "\n",
    "verification_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "jeffreys bionomial proportion is: [0.99, 0.99]\nWe examined 102 labels, of which 101 are correct.\n"
    }
   ],
   "source": [
    "# insert bionomial proprtion esimator here\n",
    "\n",
    "def print_CI(labels, response_is_standardized = \"standardized\", method = \"jeffreys\"):\n",
    "    successful_count = sum(\n",
    "        response_is_standardized == label for label in labels\n",
    "    )\n",
    "    not_examined_count = sum(\n",
    "        None == label for label in labels\n",
    "    )\n",
    "\n",
    "    CI = proportion_confint(\n",
    "            count= successful_count,\n",
    "            nobs= len(labels) - not_examined_count,\n",
    "            alpha = 0.95,\n",
    "            method=method\n",
    "        )\n",
    "    print(f\"{method} bionomial proportion is: [{CI[0]:.2f}, {CI[1]:.2f}]\",\n",
    ")\n",
    "    print(f\"We examined {len(labels) - not_examined_count} labels, of which {successful_count} are correct.\")\n",
    "print_CI(labels=verification_widget.new_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4)\n",
    "# Now we write out the verfiied results\n",
    "# ... finally we can write this out as our first complete lookup table\n",
    "# for the NAME field\n",
    "the_df['STANDARDIZEDNAME_1'] =\\\n",
    "    draft_output['MULTI_REPLACE_STANDARDIZEDNAME_1']\n",
    "\n",
    "the_df.to_csv(rootpath + interimpath + \"{}\".format(content_is),\n",
    "              index = False,\n",
    "              chunksize = 10000,\n",
    "              columns=columns_to_save)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitd4adstandardizationpipenvcac7d9f4a0864f29b6353caf0213501a",
   "display_name": "Python 3.7.2 64-bit ('d4ad_standardization': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}