
Beenw orking out of my notebook more but
I've been having issues with the jupyter server
hanging on large cell runs when prior larger runs 
ran fine. could be a buffer thing?

Anyway, added 
Host *
IPQoS=throughput

to my ssh/config
---
This gives a tasks list of sorts for 
    Standardization
        Prerequisites standardization
        Abbreviations expanded (primarily technical products and industry terms)
            carry forward into Description, FeatureDescription
        Name <-- probably need a Google call?
    Imputation (high accuracy only)



----
note: notes from meetin gare here
https://docs.google.com/document/d/1GILK30dx7Mc4v7edsfB3qYgkSG4PrrxITaV0wi3uaTI/edit
Was given VMWare scope, compared to my contract, got 

Kwamata D4AD contract
A	Prerequisite,
B	TrainRoute1,
C	NonGovApproval, 
D	(New) Instruction Modality, 
E	URL, 
F	Name_1, 
G	DOL-Type, ??? WHere is this
H	Name,
I	Address, 
J	CIP Code
K	(New) course status/type
L	(New) program status.
M	Name
(Note: (M) Name was not included but should be)
—

See: https://github.com/newjersey/d4ad/blob/master/backend/migrations/sqls/20200605145412-programs-up.sql
VMWare Scope

X, given	PROVIDERID
** M			OFFICIALNAME
J,given		CIPCODE
X,given		APPROVINGAGENCYID
X,given		OTHERAGENCY
X?			SUBMITTEDTOWIB
X,given		TUITION
X,given		FEES
X,given		BOOKSMATERIALSCOST
X,given		SUPPLIESTOOLSCOST
X,given		OTHERCOSTS
X,given		TOTALCOST
** A			PREREQUISITES
Self		WIAELIGIBLE
Self		LEADTODEGREE
DegreeID	DEGREEAWARDED
Self			LEADTOLICENSE <— Can induce license from CIP, industry requirements? Hard to say
K,L?			LICENSEAWARDED <— Unsure; maybe see if description mentions?
Self			LEADTOINDUSTRYCREDENTIAL <— Unsure; maybe see if description mentions?
IndustryID	INDUSTRYCREDENTIAL <— Can induce prerequisites, from where unsure; ONET?
Self			FINANCIALAID <— might indicate type/kind of program? Aid for cert are more rare?
** FeatureDescription, Name_1		DESCRIPTION
Credits		CREDIT
Self			TOTALCLOCKHOURS
Self			CALENDARLENGTHID
Self			FEATURESDESCRIPTION
Self?		WIBCOMMENT <— might have useful data?
Self			STATECOMMENT <— might have useful data?
Self			SUBMITTED <— might have useful data
UNK			APPROVED
Self			CONTACTNAME
Self			CONTACTPHONE
Self			PHONEEXTENSION
Self			PROGRAMID
StatusID		STATUSNAME
Self			id <— key




---
k, so next step is to stream out all the presquites as tokens
and figure out what prodigy task would allow me to efficiently assign classes to them

---
confirmed that rule set is good, I do think the double space
---
Okay, doing this again yeiled

After that examination we found 13  cases out of 83
0.8433734939759037

but we skip NaN, small_df2.PREREQUISITES.value_counts() is 73 elements
so we have 27 elements that are NaN

(110-13)/110 <-- 88

so we have average of 0.91
--
okay, so we're still above 0.90, so the evaluationis likely correct
    Also added {'IS_SPACE': True} to capture some spaces
---
what does 'later' meanl;it's a presquite but refers to prior token
'equivalent' is the same thing, remove and reach back

What does 'and' mean inthis context
'higher' is a backward reference

or (more/older/trained/etc) is defintely a backwards reference that's breaking

Asscoiates or undergraduate degree, etc

Valid NJ or PA License
    or when use as a CCONJ with a Noun following seems to describe two different things

'.  ' is defintely a token to match for/makes me almost want to do a statistical learner
---
K is on a call so I can't buy Prodigy at the moment but i can buyt it after,
but I should look into making a sampling loops to evaluate another 100

also, as a note, I happeend upon an issue where I was aware of the multiple components to the ansewr, which is good

---
Ooooookkkkay,
so one of the founders posted this flowchart for NER, https://prodi.gy/prodigy_flowchart_ner-36f76cffd9cb4ef653a21ee78659d366.pdf

that they recommend for NET bootstrapping, following that path
I end up at the node that says (after you're sure of your evaluation), Stick with rules, probably not worth training a model.
---
Following: https://www.youtube.com/watch?v=l4scwf8KeIA

starts up with seed names, then goes over a list of tokens
    starts with a match.something  prodigy method
At 10:25 starts on NER, which requires more context (much like this task)
    starts, starts with terms-to pattern file (which we may already have)
    note this is a really simpel format and I can probably export this myself

    does ner.teach <name> en_core_web_lg train --loader <corpus?> --label NER_LABEL --patterns <pattern file>.jsonl

    basic idea is to apply a small series of recipes
        terms --> bootstrap other terms
        train in context (NET) --> bootstrap model
        batch train --> train against labeled data for higher accuracy

prodigy appears to basically be a experiment label and evaluation system
---
a more modern tutorial might be here:
    * https://www.youtube.com/watch?v=59BKHO_xBPA


----
okay, sO I think I have a visually actionabel way forward

* using the matcher gui, here: https://explosion.ai/demos/matcher
* using the dep parsing graphic, with spacy.displacy.serve(nlp(conj_test), style='dep')
    * take the sample with df.sample(n=100, random_state=42)
    * on the sample, using the matcher gui, we want to write a rule, set of rules
        that tokenizes non prequites or things that don't need to be standardized
    * by doing this the left, right of tokens include content that needs to be standardized
        and may already be unabbreviated or may be abbreviated. Either case, it may be need 
        to be mapped to a canonical name
    * If I can reliably do this for this sample of 100, including custom rules like if 
        Look at https://    then skip this or flag for manual inspection, then
        it should work reliably for the entire document (or at least as a start)

key theoretical assumptions/approaches here
    * sampling can suggest suitablity for the entire thing w/o having to search more
    * that non standardized content can be identified independently, brackets standardized content
    * (C) a set of rules can reliably be used together to span a string, that is, are consistent <-- ?
    *  
----
List of lemmas: https://raw.githubusercontent.com/explosion/spacy-lookups-data/master/spacy_lookups_data/data/en_lemma_lookup.json
note the lemma "be" which is important for some reason I can't remember righ tnow
-
POS tags, note the morphology which might be important
https://spacy.io/api/annotation#pos-tagging
--
Summary of random 100
* There are several non-standardization references go across the match
    * Each seem somewhat unique

Had 6 failures out of 100, per https://epitools.ausvet.com.au/ciproportion, this suggests
a 95% CI of [0.88, 0.97] around 0.96 mean
So, then, the set of these rules should be Good-Enough

---
Work on top 100
df.sample(n=100, random_state=42)
-
X index 1 and 1 conflict, need to basically get non
conjuncting punct
        This isn't showing up in the console, it's a punct all the way around
        this means we need to specially index 1 (note this only happened once so far)

So the best soln so far is 
    {'ORTH': ','}
--
Index 30 has a prior reference across non-standardization 
"Basic Knowledge of Computers, Windows NT or 98 and database"
    Windows NT
    Windows 98 <-- could take 98 to mean [Windows 98] in the CIP context
similarly 
*? Index 38 has pior reference 
"High School Diploma [or] GED [and] Official College Transcript"
<-- HS or [GED and Transcript]
?
-
*? Note index 50
"High School Diploma/ GED, and Network+ or equivalent level of knowledge"
 where 'equivalent level of knowledge' is a big or, references prior

X index 60
 'There are no specific prerequisites for this program, but before'
? weird reference
--
* index 65
'High School Diploma or Equivalent; Reliable PC and Internet Acces'
has "Equivalent; Reliable PC" as unstandardized token
--
* See www.rate.rutgers.edu for details
    need stop case if it say www.rate.

-
goal is to figure out a rule that for all samples (100 of them) it
accurately breaks up the Prerequisites into token statements that
are or are not abbreviated. The idea is, if i canmake this work on
a sample of 100 then, per binomial proportion, it should represent
narly 100% work on the remaining data.

And, with that, I can iteratively translate the abbreviations 
from english to expanded english into their full ntotation
---

Okay, uh, this is what I want: https://explosion.ai/demos/matcher
pretty crazy they're so full featured 
see: https://spacy.io/usage/rule-based-matching

---
Okay, so last night during TV watching I stumbled upon the
EntityMatcher, see: https://spacy.io/api/entityruler

There is also the EntityRecognizer, which is a statistical approach

(and can be combined with)
see examples here: https://spacy.io/usage/rule-based-matching#entityruler


---
Here's where/how to create your own tokenizer

https://spacy.io/usage/linguistic-features#tokenization
Here's a nice explanation of the tokenization pipeline: https://spacy.io/usage/linguistic-features#101-data


I suspect I want to do mine on:
    * Symbols (&, comma, etc)
    * stopwords
-----
goal for today:
* install spacy, language model
* see if tokenization let's me split up description/target field into tokens (single, multiple words)
that need to be abbreviated and into tokens that do not need to be abbreviated.

Now, at this point, we need to include context to better learn how to unabbreviate a word (e.g. the CIP code). So this means that the abbreviation needs to be presented alogn with the CIP it came from.

The problem is that i can't read or know CIPs off the top of my head; i need the rest of the context. This suggests a kind of attention model, something. However, this process allows me to, hopefully, rapidly create abbreviation mappings that I can literally just map through.
--
 spacy.require_gpu()
 to enable gpu niceness

-
python -m spacy download en_core_web_lg
# see: https://spacy.io/models/en#en_core_web_lg

import spacy
nlp = spacy.load("en_core_web_lg")
---
Idea
With dictionary of non abbreviations can Id non abbreviations in tokenized string and then identify tokens left in a iteratively as abbreviated tokens

With tokens that need to be abbreviated identified then we can apply a method depending on the field

Active learning should accelerate abbreviation labels with actual full tokens, maybe as translation prob , then we have a giant NER mapping that we can feed back into spaCy for tagging at scale

---
Note: can see acronymn expansion problem as a character level
translation problem (which does make sense)

---
Okay,
    this appears to give active learning labeling
https://inception-project.github.io/example-projects/recommender/

----
have a moment here, going to focus on the Semantic/Keyword match

for some reason I was aware of auto-ml but not auto-ml, wow
see: auto nlp github

okay
so the first kind of matching is cross-level semantic matching, although that term has fallen
actually, no, 

CIPCODE - dataset has this filled in, one spot check looks correct. Ignore for now, somehow not needed.

NAME_1 - Program name
    Certification (A+, etc.)
    Specific Program (Microsoft Excel)
    Program Description (as opposed to its name)
    Program Name with various abbreviations present

I notice that anything that's not a word (e.g. look up in word2vec dictionary) is basically an abbreviation
Hints at a multi step filtering process with 
    So this reduces to, basically, take out anything that's not a word, figure out how to do iterative labeling on 
    that dataset, like active learning. Problem is, number of classes is either unknown or enormous.

    I also assume we'll have a lot of unique values. Perahps a better way is to extract them, sort by similarity
    and then ... learn to maybe group them?

    Also, does the description mentioning the same content indicate it's a description?

see: https://datascience.stackexchange.com/questions/28594/what-are-helpful-annotation-tools-if-any

Instruction Modality
    This is actually a more normal/true nlp problem since there are only 4 classes I think. This would
    be ammenable to the nlp annoators above

NONGOV
    So this is unique tokens, similarity grouping
    Then google them out
    I think Leslie said there were only 27 or so

Preresquite
    This is matching FeatureDescription, Description to something like Credential engine competency, presquites

--
So, overall have;
    * Identify when a token is not a word
    * Identify what tokens are the same, what tokens are the same as a word
        This might be done by permuting the word (?)
        Reminds me of tSNE

---
okay, got a sketch of the google API
    it might be enought o just fill in at a later time?
    move on to semantic?


---
Okay, trying to determine which processes to start with first, doing a tally

Semantic/Keyword match      5
External API                4
Ask Submitter/Google Form   8 (wow)
Shorten                     1
--

So techically I should start w an intelligent google form but ...
for some reason I thought Google forms weren't going to be relied upon as much.
3 fields that Google Forms also cite 3 Semantic match

So it look like, actualy, a good semantic matcher is needed? I should probably 
catpure the Google Places API call function to date and switch here. 
--
okay so thinking this through, I should probably have utils folder that includes
methods. The init can also import already created methods if they're duplicated or 
parameterized. 

Following my assessment, those methods are then used in processes, which can be done depending
on the need, so like, finte state machine (https://github.com/pytransitions/transitions#quickstart),
py rq


----
For description standardization I want to identify all acronymns and "unstandardized" language

see: https://github.com/doccano/doccano

trying to find other interactive NLP engines, though
see: https://github.com/NIHOPA/NLPre
a specific proprocessing library, w00t!

? https://github.com/philgooch/abbreviation-extraction

also, I should lead with the most accurate standardization, which is probably Google places addresses normalization

