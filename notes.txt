
okay, sO I think I have a visually actionabel way forward

* using the matcher gui, here: https://explosion.ai/demos/matcher
* using the dep parsing graphic, with spacy.displacy.serve(nlp(conj_test), style='dep')
    * take the sample with df.sample(n=100, random_state=42)
    * on the sample, using the matcher gui, we want to write a rule, set of rules
        that tokenizes non prequites or things that don't need to be standardized
    * by doing this the left, right of tokens include content that needs to be standardized
        and may already be unabbreviated or may be abbreviated. Either case, it may be need 
        to be mapped to a canonical name
    * If I can reliably do this for this sample of 100, including custom rules like if 
        Look at https://    then skip this or flag for manual inspection, then
        it should work reliably for the entire document (or at least as a start)

key theoretical approaches here
    * sampling can suggest suitablity for the entire thing w/o having to search more
    * that non standardized content can be identified independently, brackets standardized content
    * a set of rules can reliably be used together to span a string, that is, are consistent <-- ?
    *  

---
Work on top 100
df.sample(n=100, random_state=42)

goal is to figure out a rule that for all samples (100 of them) it
accurately breaks up the Prerequisites into token statements that
are or are not abbreviated. The idea is, if i canmake this work on
a sample of 100 then, per binomial proportion, it should represent
narly 100% work on the remaining data.

And, with that, I can iteratively translate the abbreviations 
from english to expanded english into their full ntotation
---

Okay, uh, this is what I want: https://explosion.ai/demos/matcher
pretty crazy they're so full featured 
see: https://spacy.io/usage/rule-based-matching

---
Okay, so last night during TV watching I stumbled upon the
EntityMatcher, see: https://spacy.io/api/entityruler

There is also the EntityRecognizer, which is a statistical approach

(and can be combined with)
see examples here: https://spacy.io/usage/rule-based-matching#entityruler


---
Here's where/how to create your own tokenizer

https://spacy.io/usage/linguistic-features#tokenization
Here's a nice explanation of the tokenization pipeline: https://spacy.io/usage/linguistic-features#101-data


I suspect I want to do mine on:
    * Symbols (&, comma, etc)
    * stopwords
-----
goal for today:
* install spacy, language model
* see if tokenization let's me split up description/target field into tokens (single, multiple words)
that need to be abbreviated and into tokens that do not need to be abbreviated.

Now, at this point, we need to include context to better learn how to unabbreviate a word (e.g. the CIP code). So this means that the abbreviation needs to be presented alogn with the CIP it came from.

The problem is that i can't read or know CIPs off the top of my head; i need the rest of the context. This suggests a kind of attention model, something. However, this process allows me to, hopefully, rapidly create abbreviation mappings that I can literally just map through.
--
 spacy.require_gpu()
 to enable gpu niceness

-
python -m spacy download en_core_web_lg
# see: https://spacy.io/models/en#en_core_web_lg

import spacy
nlp = spacy.load("en_core_web_lg")
---
Idea
With dictionary of non abbreviations can Id non abbreviations in tokenized string and then identify tokens left in a iteratively as abbreviated tokens

With tokens that need to be abbreviated identified then we can apply a method depending on the field

Active learning should accelerate abbreviation labels with actual full tokens, maybe as translation prob , then we have a giant NER mapping that we can feed back into spaCy for tagging at scale

---
Note: can see acronymn expansion problem as a character level
translation problem (which does make sense)

---
Okay,
    this appears to give active learning labeling
https://inception-project.github.io/example-projects/recommender/

----
have a moment here, going to focus on the Semantic/Keyword match

for some reason I was aware of auto-ml but not auto-ml, wow
see: auto nlp github

okay
so the first kind of matching is cross-level semantic matching, although that term has fallen
actually, no, 

CIPCODE - dataset has this filled in, one spot check looks correct. Ignore for now, somehow not needed.

NAME_1 - Program name
    Certification (A+, etc.)
    Specific Program (Microsoft Excel)
    Program Description (as opposed to its name)
    Program Name with various abbreviations present

I notice that anything that's not a word (e.g. look up in word2vec dictionary) is basically an abbreviation
Hints at a multi step filtering process with 
    So this reduces to, basically, take out anything that's not a word, figure out how to do iterative labeling on 
    that dataset, like active learning. Problem is, number of classes is either unknown or enormous.

    I also assume we'll have a lot of unique values. Perahps a better way is to extract them, sort by similarity
    and then ... learn to maybe group them?

    Also, does the description mentioning the same content indicate it's a description?

see: https://datascience.stackexchange.com/questions/28594/what-are-helpful-annotation-tools-if-any

Instruction Modality
    This is actually a more normal/true nlp problem since there are only 4 classes I think. This would
    be ammenable to the nlp annoators above

NONGOV
    So this is unique tokens, similarity grouping
    Then google them out
    I think Leslie said there were only 27 or so

Preresquite
    This is matching FeatureDescription, Description to something like Credential engine competency, presquites

--
So, overall have;
    * Identify when a token is not a word
    * Identify what tokens are the same, what tokens are the same as a word
        This might be done by permuting the word (?)
        Reminds me of tSNE

---
okay, got a sketch of the google API
    it might be enought o just fill in at a later time?
    move on to semantic?


---
Okay, trying to determine which processes to start with first, doing a tally

Semantic/Keyword match      5
External API                4
Ask Submitter/Google Form   8 (wow)
Shorten                     1
--

So techically I should start w an intelligent google form but ...
for some reason I thought Google forms weren't going to be relied upon as much.
3 fields that Google Forms also cite 3 Semantic match

So it look like, actualy, a good semantic matcher is needed? I should probably 
catpure the Google Places API call function to date and switch here. 
--
okay so thinking this through, I should probably have utils folder that includes
methods. The init can also import already created methods if they're duplicated or 
parameterized. 

Following my assessment, those methods are then used in processes, which can be done depending
on the need, so like, finte state machine (https://github.com/pytransitions/transitions#quickstart),
py rq


----
For description standardization I want to identify all acronymns and "unstandardized" language

see: https://github.com/doccano/doccano

trying to find other interactive NLP engines, though
see: https://github.com/NIHOPA/NLPre
a specific proprocessing library, w00t!

? https://github.com/philgooch/abbreviation-extraction

also, I should lead with the most accurate standardization, which is probably Google places addresses normalization

